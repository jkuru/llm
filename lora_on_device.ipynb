{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why LoRA is Better for On-Device LLMs\n",
    "\n",
    "*A companion to the [KV Cache Walkthrough](kv_cache_always_on.ipynb) — connecting what we built in Chapters 3-4 to real-world on-device deployment.*\n",
    "\n",
    "In the KV cache notebook, we learned how to make **inference fast** by avoiding redundant computation. But there's another efficiency challenge: how do you **customize** a model for a specific task (like your personal writing style, a medical domain, or a specific language) without retraining all 124 million parameters?\n",
    "\n",
    "This is where **LoRA (Low-Rank Adaptation)** comes in — and it's especially important for on-device deployment (phones, laptops, edge hardware).\n",
    "\n",
    "> **How to use this notebook:** Run the cells top-to-bottom (`Shift+Enter`). We'll build a working LoRA implementation from scratch using the same GPT model from the KV cache notebook, then compare parameter counts and see LoRA in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Full Fine-Tuning is Expensive\n",
    "\n",
    "When you fine-tune a model the traditional way, you update **every parameter** in every weight matrix:\n",
    "\n",
    "```\n",
    "GPT-2 124M has these weight matrices (among others):\n",
    "  W_query:  (768 × 768) = 589,824 parameters    ← update ALL of these\n",
    "  W_key:    (768 × 768) = 589,824 parameters    ← update ALL of these\n",
    "  W_value:  (768 × 768) = 589,824 parameters    ← update ALL of these\n",
    "  FeedForward layer 1: (768 × 3072) = 2,359,296 ← update ALL of these\n",
    "  ...\n",
    "  Total: ~124 million parameters to store, update, and deploy\n",
    "\n",
    "For a 7B model: 7 BILLION parameters × 4 bytes each = 28 GB just for the weights!\n",
    "```\n",
    "\n",
    "For on-device deployment, this is a non-starter. You can't store a separate 28 GB model for every task — your phone only has 6-8 GB of RAM total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA's Key Insight: Weight Changes Are Low-Rank\n",
    "\n",
    "When you fine-tune a model, the **change** to each weight matrix (ΔW) turns out to be surprisingly low-rank. This means you can approximate ΔW as the product of two much smaller matrices:\n",
    "\n",
    "```\n",
    "Full fine-tuning:\n",
    "  W_new = W_original + ΔW                    ← ΔW is (768 × 768) = 589,824 values\n",
    "\n",
    "LoRA:\n",
    "  W_new = W_original + (A × B)               ← A is (768 × r), B is (r × 768)\n",
    "                                                 where r = \"rank\" (typically 4-16)\n",
    "\n",
    "With rank r = 8:\n",
    "  A: (768 × 8)  =  6,144 parameters\n",
    "  B: (8 × 768)  =  6,144 parameters\n",
    "  Total:           12,288 parameters          ← that's ~2% of the original 589,824!\n",
    "```\n",
    "\n",
    "### Why is ΔW low-rank?\n",
    "\n",
    "Think of it this way: when you fine-tune a medical chatbot from a general-purpose model, you're not changing *everything* the model knows. You're mostly adjusting a specific *direction* in the weight space — \"pay more attention to medical terminology\" or \"format answers like a doctor would.\" These task-specific adjustments tend to live in a low-dimensional subspace of the full weight matrix, which is exactly what the low-rank A × B decomposition captures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual: How LoRA Modifies a Linear Layer\n",
    "\n",
    "```\n",
    "                           Standard Fine-Tuning          LoRA\n",
    "                           ────────────────────          ────\n",
    "\n",
    "Input x ──→ [ W_original + ΔW ] ──→ output    Input x ──┬──→ [ W_original ] ──→  +  ──→ output\n",
    "             (768 × 768)                                  │     (768 × 768)        ↑\n",
    "             589,824 new params                           │     (FROZEN!)          │\n",
    "                                                          │                        │\n",
    "                                                          └──→ [ A ]──→[ B ]──────┘\n",
    "                                                               (768×8)  (8×768)\n",
    "                                                               12,288 new params\n",
    "                                                               (TRAINABLE)\n",
    "```\n",
    "\n",
    "The original weight `W_original` stays **frozen** (unchanged). Only the tiny A and B matrices are trained. This is the magic of LoRA.\n",
    "\n",
    "During inference, you can even **merge** A × B back into the original weight: `W_merged = W_original + A × B`. This means zero extra latency at inference time — the model runs at the exact same speed as the original, with no extra computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Build It\n",
    "\n",
    "Here's a working LoRA layer that wraps any `nn.Linear`. We'll then apply it to our GPT model's attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"A Linear layer with a LoRA adapter attached.\n",
    "    \n",
    "    The original weight W is frozen. Two small matrices A and B are trainable.\n",
    "    Output = W(x) + B(A(x)) * scaling_factor\n",
    "    \n",
    "    Args:\n",
    "        original_linear: The nn.Linear layer to wrap (will be frozen)\n",
    "        rank:            The LoRA rank (r) — smaller = fewer params, larger = more expressive\n",
    "        alpha:           Scaling factor — controls how much LoRA affects the output\n",
    "                         (the actual scale applied is alpha/rank)\n",
    "    \"\"\"\n",
    "    def __init__(self, original_linear, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original = original_linear\n",
    "        in_features = original_linear.in_features    # e.g., 768\n",
    "        out_features = original_linear.out_features  # e.g., 768\n",
    "        \n",
    "        # Freeze the original weight — we don't want gradients flowing to it\n",
    "        self.original.weight.requires_grad = False\n",
    "        if self.original.bias is not None:\n",
    "            self.original.bias.requires_grad = False\n",
    "        \n",
    "        # LoRA matrices:\n",
    "        # A: projects DOWN from input dimension to rank  (768 → 8)\n",
    "        # B: projects UP from rank to output dimension   (8 → 768)\n",
    "        # Their product A × B approximates the weight change ΔW\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=False)   # (768, 8)\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=False)  # (8, 768)\n",
    "        \n",
    "        # Initialize A with small random values (Kaiming uniform, same as the LoRA paper)\n",
    "        # Initialize B with zeros — so at the start, LoRA has NO effect (A×B = 0)\n",
    "        # This means the model starts from its pre-trained behavior\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "        # Scaling factor: alpha / rank\n",
    "        # Higher alpha = LoRA has more influence on the output\n",
    "        # Dividing by rank keeps the scale consistent regardless of rank choice\n",
    "        self.scaling = alpha / rank\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Original path: W(x) — frozen, no gradients\n",
    "        original_output = self.original(x)\n",
    "        \n",
    "        # LoRA path: B(A(x)) * scaling — trainable!\n",
    "        # x → A → (768→8) → B → (8→768) → scale\n",
    "        lora_output = self.lora_B(self.lora_A(x)) * self.scaling\n",
    "        \n",
    "        # Combined: original + LoRA adjustment\n",
    "        return original_output + lora_output\n",
    "    \n",
    "    def merge(self):\n",
    "        \"\"\"Merge LoRA weights into the original weight for zero-overhead inference.\n",
    "        \n",
    "        After merging: W_new = W_original + (B × A) * scaling\n",
    "        The model runs at the same speed as the original — no extra computation.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # A.weight: (rank, in_features)\n",
    "            # B.weight: (out_features, rank)\n",
    "            # B × A:    (out_features, in_features) — same shape as W!\n",
    "            delta_w = self.lora_B.weight @ self.lora_A.weight * self.scaling\n",
    "            self.original.weight.add_(delta_w)\n",
    "        return self.original  # Return the merged layer (no LoRA overhead)\n",
    "\n",
    "\n",
    "print(\"LoRALinear class defined!\")\n",
    "print(\"\\nKey design choices:\")\n",
    "print(\"  - B initialized to zeros → LoRA starts with no effect\")\n",
    "print(\"  - scaling = alpha/rank → consistent impact regardless of rank\")\n",
    "print(\"  - merge() → fold LoRA into weights for zero-overhead inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count: Full Fine-Tuning vs LoRA\n",
    "\n",
    "Let's see the numbers with a real GPT-2 sized layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standard Linear layer (like W_query in our GPT model)\n",
    "d_model = 768  # GPT-2 embedding dimension\n",
    "original_layer = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "full_params = sum(p.numel() for p in original_layer.parameters())\n",
    "print(f\"Original W_query layer: {full_params:,} parameters\")\n",
    "print(f\"  Shape: ({d_model} × {d_model})\")\n",
    "print()\n",
    "\n",
    "# Now wrap it with LoRA at different ranks\n",
    "for rank in [1, 4, 8, 16, 32, 64]:\n",
    "    lora_layer = LoRALinear(nn.Linear(d_model, d_model, bias=False), rank=rank)\n",
    "    \n",
    "    # Count only the TRAINABLE parameters (A and B matrices)\n",
    "    trainable = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in lora_layer.parameters())\n",
    "    pct = trainable / total * 100\n",
    "    \n",
    "    print(f\"  LoRA rank {rank:2d}: {trainable:>8,} trainable params \"\n",
    "          f\"({pct:5.2f}% of total)  \"\n",
    "          f\"[A: ({d_model}×{rank}), B: ({rank}×{d_model})]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying LoRA to a GPT Model's Attention Layers\n",
    "\n",
    "In practice, LoRA is typically applied to the attention projection matrices (`W_query`, `W_key`, `W_value`, and sometimes `out_proj`). Let's see how many trainable parameters we'd have versus full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    \"\"\"Count total and trainable parameters in a model.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "def apply_lora_to_attention(model, rank=8, alpha=16):\n",
    "    \"\"\"Replace W_query and W_value in every attention layer with LoRA-wrapped versions.\n",
    "    \n",
    "    Following the original LoRA paper, we apply LoRA to W_query and W_value.\n",
    "    (Some implementations also include W_key and out_proj for more expressiveness.)\n",
    "    \"\"\"\n",
    "    lora_count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        # Find MultiHeadAttention modules\n",
    "        if hasattr(module, 'W_query') and hasattr(module, 'W_value'):\n",
    "            # Wrap W_query with LoRA\n",
    "            module.W_query = LoRALinear(module.W_query, rank=rank, alpha=alpha)\n",
    "            # Wrap W_value with LoRA  \n",
    "            module.W_value = LoRALinear(module.W_value, rank=rank, alpha=alpha)\n",
    "            lora_count += 2\n",
    "    \n",
    "    # Freeze ALL parameters first\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Then unfreeze ONLY LoRA parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora_' in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    return lora_count\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")\n",
    "print(\"apply_lora_to_attention() wraps W_query and W_value in every attention layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a GPT-2 124M model (same config as the KV cache notebook)\n",
    "# We'll import the model class from the book's code\n",
    "import sys\n",
    "sys.path.insert(0, '../../ch04/01_main-chapter-code')\n",
    "\n",
    "# Use a simpler model definition inline for self-containment\n",
    "class SimpleMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Simplified MHA without KV cache — just for demonstrating LoRA param counts.\"\"\"\n",
    "    def __init__(self, d_in, d_out, num_heads, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "class SimpleGPT(nn.Module):\n",
    "    \"\"\"Minimal GPT structure for demonstrating LoRA parameter savings.\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        \n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(cfg['n_layers']):\n",
    "            block = nn.Module()\n",
    "            block.att = SimpleMultiHeadAttention(\n",
    "                d_in=cfg['emb_dim'], d_out=cfg['emb_dim'],\n",
    "                num_heads=cfg['n_heads'], qkv_bias=cfg.get('qkv_bias', False)\n",
    "            )\n",
    "            block.ff = nn.Sequential(\n",
    "                nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'])\n",
    "            )\n",
    "            block.norm1 = nn.LayerNorm(cfg['emb_dim'])\n",
    "            block.norm2 = nn.LayerNorm(cfg['emb_dim'])\n",
    "            self.blocks.append(block)\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "\n",
    "# GPT-2 124M config\n",
    "GPT_CONFIG = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'qkv_bias': False,\n",
    "}\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = SimpleGPT(GPT_CONFIG)\n",
    "\n",
    "total_before, trainable_before = count_params(model)\n",
    "print(f\"=== Before LoRA (Full Fine-Tuning) ===\")\n",
    "print(f\"  Total parameters:     {total_before:>12,}\")\n",
    "print(f\"  Trainable parameters: {trainable_before:>12,}  (100%)\")\n",
    "print()\n",
    "\n",
    "# Apply LoRA\n",
    "num_lora = apply_lora_to_attention(model, rank=8, alpha=16)\n",
    "\n",
    "total_after, trainable_after = count_params(model)\n",
    "print(f\"=== After LoRA (rank=8) ===\")\n",
    "print(f\"  Total parameters:     {total_after:>12,}\")\n",
    "print(f\"  Trainable parameters: {trainable_after:>12,}  ({trainable_after/total_after*100:.2f}%)\")\n",
    "print(f\"  Frozen parameters:    {total_after - trainable_after:>12,}\")\n",
    "print(f\"  LoRA layers added:    {num_lora} (W_query + W_value × {num_lora//2} blocks)\")\n",
    "print()\n",
    "print(f\"  Reduction: {trainable_before:,} → {trainable_after:,} trainable params\")\n",
    "print(f\"  That's {trainable_after/trainable_before*100:.2f}% of the original — \"\n",
    "      f\"a {trainable_before/trainable_after:.0f}x reduction!\")\n",
    "print()\n",
    "\n",
    "# Storage comparison\n",
    "full_mb = trainable_before * 4 / (1024**2)   # 4 bytes per float32 param\n",
    "lora_mb = trainable_after * 4 / (1024**2)\n",
    "print(f\"  Storage for fine-tuned weights:\")\n",
    "print(f\"    Full fine-tune: {full_mb:.1f} MB\")\n",
    "print(f\"    LoRA adapter:   {lora_mb:.2f} MB\")\n",
    "print(f\"    Savings:        {full_mb - lora_mb:.1f} MB per task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA in Action: Before and After Merge\n",
    "\n",
    "Let's verify that:\n",
    "1. LoRA starts with zero effect (B is initialized to zeros)\n",
    "2. After simulating some training, the output changes\n",
    "3. After merging, the output is identical but with no LoRA overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo with a single layer\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an original linear layer\n",
    "original = nn.Linear(768, 768, bias=False)\n",
    "\n",
    "# Create a test input\n",
    "x = torch.randn(1, 4, 768)  # batch=1, seq_len=4, dim=768\n",
    "\n",
    "# Output BEFORE LoRA\n",
    "with torch.no_grad():\n",
    "    out_original = original(x)\n",
    "\n",
    "# Wrap with LoRA\n",
    "lora_layer = LoRALinear(original, rank=8, alpha=16)\n",
    "\n",
    "# Output immediately after wrapping (B=0, so LoRA has NO effect)\n",
    "with torch.no_grad():\n",
    "    out_with_lora_init = lora_layer(x)\n",
    "\n",
    "# Verify: output should be IDENTICAL (LoRA starts at zero)\n",
    "diff_init = (out_original - out_with_lora_init).abs().max().item()\n",
    "print(f\"Difference after LoRA init (should be ~0): {diff_init:.10f}\")\n",
    "print(f\"  → LoRA has NO effect at initialization (B=0) ✓\")\n",
    "print()\n",
    "\n",
    "# Simulate training: modify LoRA weights as if we trained them\n",
    "with torch.no_grad():\n",
    "    lora_layer.lora_A.weight.normal_(0, 0.01)\n",
    "    lora_layer.lora_B.weight.normal_(0, 0.01)\n",
    "\n",
    "# Output after \"training\" — should be DIFFERENT from original\n",
    "with torch.no_grad():\n",
    "    out_with_lora_trained = lora_layer(x)\n",
    "\n",
    "diff_trained = (out_original - out_with_lora_trained).abs().max().item()\n",
    "print(f\"Difference after LoRA training: {diff_trained:.6f}\")\n",
    "print(f\"  → LoRA is now modifying the output ✓\")\n",
    "print()\n",
    "\n",
    "# Now MERGE LoRA into the original weight\n",
    "merged_layer = lora_layer.merge()\n",
    "\n",
    "# Output from merged layer — should be IDENTICAL to LoRA output\n",
    "with torch.no_grad():\n",
    "    out_merged = merged_layer(x)\n",
    "\n",
    "diff_merged = (out_with_lora_trained - out_merged).abs().max().item()\n",
    "print(f\"Difference after merge (should be ~0): {diff_merged:.10f}\")\n",
    "print(f\"  → Merged output matches LoRA output exactly ✓\")\n",
    "print()\n",
    "print(\"After merging:\")\n",
    "print(f\"  - The original layer now includes the LoRA adjustment\")\n",
    "print(f\"  - No extra A, B matrices needed at inference time\")\n",
    "print(f\"  - Zero additional computation or memory overhead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters for On-Device Deployment\n",
    "\n",
    "On a phone or laptop, you have severe constraints: limited RAM, limited storage, limited compute, and often no GPU. LoRA solves all of these:\n",
    "\n",
    "```\n",
    "                               Full Fine-Tune     LoRA (rank 8)\n",
    "                               ──────────────     ─────────────\n",
    "Storage per task (7B model):   28 GB              ~18 MB           ← 1500x smaller!\n",
    "RAM during inference:          28 GB              28 GB base       ← same base model\n",
    "                                                   + 18 MB adapter   shared across tasks\n",
    "Can swap tasks instantly?      No (reload model)  Yes (swap adapter)\n",
    "Can run multiple tasks?        No (one model)     Yes (one model + many adapters)\n",
    "Training data needed:          Large              Small\n",
    "Training time:                 Hours/days         Minutes/hours\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multi-Task Advantage\n",
    "\n",
    "Here's where LoRA really shines on-device. Instead of storing separate full models for each task, you store **one base model** and multiple tiny LoRA adapters:\n",
    "\n",
    "```\n",
    "Traditional approach (one model per task):\n",
    "  ┌─────────────────────┐\n",
    "  │ Medical Model  28 GB │\n",
    "  │ Legal Model    28 GB │\n",
    "  │ Coding Model   28 GB │\n",
    "  │ Chat Model     28 GB │\n",
    "  └─────────────────────┘\n",
    "  Total: 112 GB            ← impossible on a phone!\n",
    "\n",
    "LoRA approach (one base + adapters):\n",
    "  ┌─────────────────────┐\n",
    "  │ Base Model     28 GB │ ← loaded once, shared\n",
    "  │ Medical LoRA   18 MB │ ← swap in/out instantly\n",
    "  │ Legal LoRA     18 MB │\n",
    "  │ Coding LoRA    18 MB │\n",
    "  │ Chat LoRA      18 MB │\n",
    "  └─────────────────────┘\n",
    "  Total: ~28.07 GB         ← fits on device!\n",
    "```\n",
    "\n",
    "Swapping between tasks is nearly instant — you just load a different set of A and B matrices and add them to the frozen base weights. No reloading the entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LoRA Connects to the KV Cache\n",
    "\n",
    "Look at the `W_query`, `W_key`, and `W_value` matrices in the `MultiHeadAttention` class from our [KV cache notebook](kv_cache_always_on.ipynb):\n",
    "\n",
    "```python\n",
    "self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)  # (768, 768) = 589,824 params\n",
    "self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)  # (768, 768) = 589,824 params\n",
    "self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)  # (768, 768) = 589,824 params\n",
    "```\n",
    "\n",
    "With LoRA applied to W_query, the forward pass changes from:\n",
    "```python\n",
    "queries = self.W_query(x)                                    # original\n",
    "```\n",
    "to:\n",
    "```python\n",
    "queries = self.W_query(x) + self.lora_B(self.lora_A(x))     # with LoRA\n",
    "#          ↑ frozen original    ↑ trainable LoRA correction\n",
    "```\n",
    "\n",
    "The KV cache works **exactly the same** with LoRA — the queries, keys, and values that go into the cache are just slightly adjusted by the adapter. Everything else we learned (prefill, decode, causal masking, sliding window) is unchanged.\n",
    "\n",
    "And remember: after training, you can **merge** the LoRA weights back in, so at inference time the model is identical in structure to the original — the KV cache never even knows LoRA was involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete On-Device Stack: LoRA + KV Cache + Quantization\n",
    "\n",
    "In production on-device LLMs (like Apple Intelligence, Google Gemini Nano, Samsung Galaxy AI), you'll typically see all three optimizations working together:\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                    On-Device LLM Stack                       │\n",
    "│                                                              │\n",
    "│  1. QUANTIZATION (Ch 6+)                                     │\n",
    "│     Shrink model from 28 GB → 3-4 GB                         │\n",
    "│     by using 4-bit integers instead of 32-bit floats         │\n",
    "│     (each parameter: 4 bytes → 0.5 bytes)                    │\n",
    "│                                                              │\n",
    "│  2. LoRA ADAPTERS (this notebook!)                           │\n",
    "│     Customize for tasks without modifying the base model     │\n",
    "│     18 MB per task instead of 3-4 GB                         │\n",
    "│     Swap adapters instantly for different use cases           │\n",
    "│                                                              │\n",
    "│  3. KV CACHE (the KV cache notebook!)                        │\n",
    "│     Generate tokens fast without recomputing old K,V         │\n",
    "│     Sliding window to fit in limited device RAM              │\n",
    "│     (our window_size parameter controls this)                │\n",
    "│                                                              │\n",
    "│  Result: A 7B-parameter model running on your phone,         │\n",
    "│  customizable for different tasks, generating text           │\n",
    "│  at interactive speeds — all in ~4 GB of RAM                 │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "LoRA is better for on-device because it solves the fundamental tension between **model quality** (bigger models are better) and **device constraints** (phones have limited RAM/storage). Instead of making the model smaller (which hurts quality), LoRA lets you keep the full model and make task-specific adjustments through tiny, swappable adapters. Combined with the KV cache we implemented in the companion notebook and quantization (covered later in the book), this forms the complete stack that makes modern on-device AI possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}