{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT with KV Cache — A Beginner's Walkthrough\n",
    "\n",
    "*Based on Sebastian Raschka's [\"Build a Large Language Model From Scratch\"](https://www.manning.com/books/build-a-large-language-model-from-scratch) — Chapters 1 through 4.*\n",
    "\n",
    "This notebook brings together concepts from every chapter so far and adds the **KV Cache** optimization that makes real-world LLM inference fast:\n",
    "\n",
    "| Chapter | What we learned | Where it shows up here |\n",
    "|---------|----------------|----------------------|\n",
    "| **Ch 1** | How LLMs work at a high level | The overall generate → predict → append loop |\n",
    "| **Ch 2** | Tokenization (BPE) and data loading | `tiktoken` converts text ↔ token IDs |\n",
    "| **Ch 3** | The self-attention mechanism | `MultiHeadAttention` — now enhanced with a KV cache |\n",
    "| **Ch 4** | The full GPT architecture | `GPTModel` = embeddings + transformer blocks + output head |\n",
    "\n",
    "> **How to use this notebook:** Run the cells top-to-bottom (`Shift+Enter`). The model uses random weights (not pretrained), so generated text will be gibberish — that's expected! The goal is to understand the **architecture and caching mechanism**, not to produce meaningful text. Training comes in later chapters.\n",
    "\n",
    "## So what exactly is a KV Cache?\n",
    "\n",
    "When a GPT model generates text, it produces **one token at a time**. Without a cache, every time we generate a new token, we have to reprocess the *entire* sequence from scratch — that's a lot of wasted computation!\n",
    "\n",
    "**The KV Cache stores the Keys and Values** from previous tokens so we never recompute them. Think of it like taking notes during a lecture — instead of re-listening to everything from the start each time you want to write the next sentence, you just look at your notes (the cache) and only process the new information.\n",
    "\n",
    "### With KV cache (fast):\n",
    "```\n",
    "Step 1 (Prefill):  \"Hello, I am\" → process all 4 tokens, cache their K,V\n",
    "Step 2 (Decode):   \" a\"          → process ONLY this 1 token, attend to cached K,V\n",
    "Step 3 (Decode):   \" large\"      → process ONLY this 1 token, attend to cached K,V\n",
    "...each step is O(1) new computation\n",
    "```\n",
    "\n",
    "### Without cache (slow):\n",
    "```\n",
    "Step 1: \"Hello, I am\"           → process 4 tokens\n",
    "Step 2: \"Hello, I am a\"         → reprocess ALL 5 tokens from scratch\n",
    "Step 3: \"Hello, I am a large\"   → reprocess ALL 6 tokens from scratch\n",
    "...each step gets slower!\n",
    "```\n",
    "\n",
    "This is a simplified version of `gpt_with_kv_cache.py` where the cache is **always on** — no toggle flags — so you can focus purely on understanding the mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time       # To measure how fast generation is\n",
    "import tiktoken   # OpenAI's tokenizer — converts text ↔ token IDs\n",
    "import torch      # PyTorch — the deep learning framework\n",
    "import torch.nn as nn  # Neural network building blocks (Linear, Embedding, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadAttention with KV Cache\n",
    "\n",
    "This is the core of the KV cache mechanism. Here's the key insight:\n",
    "\n",
    "In standard attention (no cache), **every forward pass** computes Keys, Values, and Queries for **all** tokens in the input. During generation, this means recomputing K,V for tokens we've already seen.\n",
    "\n",
    "With a KV cache:\n",
    "- We **only** compute K, V, Q for the **new** token(s) coming in\n",
    "- We **append** the new K, V to a stored cache\n",
    "- Queries attend to **all** cached Keys/Values (old + new)\n",
    "\n",
    "### Visual: What happens at each decode step\n",
    "\n",
    "```\n",
    "Cache before:  [K₀, K₁, K₂]     (keys from previous tokens)\n",
    "New input:     [K₃]              (key from the new token)\n",
    "Cache after:   [K₀, K₁, K₂, K₃] (append the new key)\n",
    "\n",
    "Query Q₃ attends to → [K₀, K₁, K₂, K₃]  (full history!)\n",
    "```\n",
    "\n",
    "### The \"window\" concept\n",
    "\n",
    "The cache has a fixed size (`window_size`). If it fills up, the oldest entries are discarded (shifted out). This is like a sliding window over the conversation history.\n",
    "\n",
    "```\n",
    "Window size = 4\n",
    "Cache: [K₀, K₁, K₂, K₃]  ← full!\n",
    "New token K₄ arrives...\n",
    "Cache: [K₁, K₂, K₃, K₄]  ← K₀ was discarded (shifted left)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False, window_size=None):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Each head works on a slice of the embedding\n",
    "\n",
    "        # These are the learned projection matrices that transform input into Q, K, V\n",
    "        # Think of them as \"what am I looking for?\" (Q), \"what do I contain?\" (K), \"what do I offer?\" (V)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Combines all heads back together\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # ========== KV CACHE SETUP ==========\n",
    "        # window_size = how many tokens the cache can hold at once\n",
    "        # cache_k, cache_v = the actual storage tensors (initialized to None, created on first use)\n",
    "        self.window_size = window_size or context_length\n",
    "        self.register_buffer(\"cache_k\", None, persistent=False)  # persistent=False → not saved to disk\n",
    "        self.register_buffer(\"cache_v\", None, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # b = batch size, num_tokens = how many NEW tokens we're processing this call\n",
    "        # During prefill: num_tokens = length of prompt (e.g., 4 for \"Hello, I am\")\n",
    "        # During decode:  num_tokens = 1 (just the latest generated token)\n",
    "\n",
    "        assert num_tokens <= self.window_size, (\n",
    "            f\"Input chunk size ({num_tokens}) exceeds KV cache window size ({self.window_size}).\"\n",
    "        )\n",
    "\n",
    "        # Step 1: Project the NEW input tokens into keys, values, queries\n",
    "        # These are ONLY for the new tokens — we don't recompute old ones!\n",
    "        keys_new = self.W_key(x)       # (b, num_tokens, d_out)\n",
    "        values_new = self.W_value(x)   # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)      # (b, num_tokens, d_out)\n",
    "\n",
    "        # Reshape to split into multiple heads:\n",
    "        # (b, num_tokens, d_out) → (b, num_tokens, num_heads, head_dim) → (b, num_heads, num_tokens, head_dim)\n",
    "        # The transpose puts num_heads before num_tokens so each head can process independently\n",
    "        keys_new = keys_new.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values_new = values_new.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # ========== STEP 2: APPEND NEW K,V TO THE CACHE ==========\n",
    "\n",
    "        # First call? Create the cache tensors (pre-allocated to window_size for efficiency)\n",
    "        if self.cache_k is None or self.cache_k.size(0) != b:\n",
    "            # Shape: (batch, num_heads, window_size, head_dim)\n",
    "            # The cache is like a fixed-size array with a write pointer (ptr_cur)\n",
    "            self.cache_k = torch.zeros(\n",
    "                b, self.num_heads, self.window_size, self.head_dim, device=x.device\n",
    "            )\n",
    "            self.cache_v = torch.zeros_like(self.cache_k)\n",
    "            self.ptr_cur = 0  # ptr_cur = \"next free slot\" in the cache\n",
    "            # Example: ptr_cur=0 means cache is empty\n",
    "            #          ptr_cur=3 means slots 0,1,2 are filled\n",
    "\n",
    "        # Would the new tokens overflow the cache? If so, shift left to make room\n",
    "        # This discards the OLDEST tokens (like a sliding window)\n",
    "        if self.ptr_cur + num_tokens > self.window_size:\n",
    "            overflow = self.ptr_cur + num_tokens - self.window_size\n",
    "            # Shift everything left by 'overflow' positions\n",
    "            # Before: [tok0, tok1, tok2, tok3, tok4] with overflow=2\n",
    "            # After:  [tok2, tok3, tok4, ___, ___]   (tok0, tok1 discarded)\n",
    "            self.cache_k[:, :, :-overflow, :] = self.cache_k[:, :, overflow:, :].clone()\n",
    "            self.cache_v[:, :, :-overflow, :] = self.cache_v[:, :, overflow:, :].clone()\n",
    "            self.ptr_cur -= overflow\n",
    "\n",
    "        # Write the new keys and values into the cache at the current pointer position\n",
    "        self.cache_k[:, :, self.ptr_cur:self.ptr_cur + num_tokens, :] = keys_new\n",
    "        self.cache_v[:, :, self.ptr_cur:self.ptr_cur + num_tokens, :] = values_new\n",
    "        self.ptr_cur += num_tokens  # Advance the pointer past what we just wrote\n",
    "\n",
    "        # ========== STEP 3: READ ALL CACHED K,V FOR ATTENTION ==========\n",
    "        # Slice from 0 to ptr_cur → this is ALL the keys/values we've seen so far\n",
    "        keys = self.cache_k[:, :, :self.ptr_cur, :]    # (b, num_heads, total_cached, head_dim)\n",
    "        values = self.cache_v[:, :, :self.ptr_cur, :]   # (b, num_heads, total_cached, head_dim)\n",
    "\n",
    "        # ========== STEP 4: COMPUTE ATTENTION SCORES ==========\n",
    "        # queries shape: (b, num_heads, num_tokens, head_dim)   ← only NEW tokens\n",
    "        # keys shape:    (b, num_heads, total_cached, head_dim) ← ALL cached tokens\n",
    "        # Result:        (b, num_heads, num_tokens, total_cached)\n",
    "        # Each new query gets a score against EVERY cached key\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        K = attn_scores.size(-1)  # K = total number of cached tokens\n",
    "\n",
    "        # ========== STEP 5: APPLY CAUSAL MASK ==========\n",
    "        # Causal mask ensures a token can only attend to tokens that came BEFORE it\n",
    "        # (no peeking at the future!)\n",
    "        #\n",
    "        # The tricky part with a cache: our queries start at position 'offset' in the sequence,\n",
    "        # not at position 0. So we need to shift the mask accordingly.\n",
    "        #\n",
    "        # Example: cache has [tok0, tok1, tok2] and we're processing [tok3]\n",
    "        #   offset = 3 (3 tokens were cached before this one)\n",
    "        #   tok3 (query row 0) can attend to positions 0,1,2,3 → all of them\n",
    "        #   So causal_mask is all False → no masking needed (tok3 can see everything before it)\n",
    "        #\n",
    "        # Example: prefill with [tok0, tok1, tok2] (no prior cache)\n",
    "        #   offset = 0\n",
    "        #   tok0 (row 0) can attend to [tok0] only\n",
    "        #   tok1 (row 1) can attend to [tok0, tok1]\n",
    "        #   tok2 (row 2) can attend to [tok0, tok1, tok2]\n",
    "        offset = K - num_tokens\n",
    "        row_idx = torch.arange(num_tokens, device=x.device).unsqueeze(1)  # (num_tokens, 1)\n",
    "        col_idx = torch.arange(K, device=x.device).unsqueeze(0)           # (1, K)\n",
    "        causal_mask = (row_idx + offset) < col_idx  # True = \"block this position\" (can't see the future)\n",
    "\n",
    "        # Replace masked positions with -infinity so softmax gives them 0 weight\n",
    "        attn_scores.masked_fill_(causal_mask.unsqueeze(0).unsqueeze(0), -torch.inf)\n",
    "\n",
    "        # ========== STEP 6: SOFTMAX + WEIGHTED SUM ==========\n",
    "        # Scale by sqrt(head_dim) to keep gradients stable, then softmax to get attention weights\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Multiply attention weights by values to get the output\n",
    "        # (b, num_heads, num_tokens, K) @ (b, num_heads, K, head_dim) → (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine all heads back into a single vector per token\n",
    "        # (b, num_tokens, num_heads, head_dim) → (b, num_tokens, d_out)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # Final linear projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "    def reset_cache(self):\n",
    "        \"\"\"Clear the cache — call this before starting a new generation.\"\"\"\n",
    "        self.cache_k, self.cache_v = None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait — What Does the Cache Actually Save?\n",
    "\n",
    "A common misconception is that the KV cache skips the attention computation entirely after the first call. **It doesn't.** The dot product (Q × K) and the weighted sum (weights × V) happen **every single call**. Here's what actually changes:\n",
    "\n",
    "### Without cache (old way) — generating the 5th token\n",
    "\n",
    "The model reprocesses the **entire** sequence from scratch:\n",
    "\n",
    "```\n",
    "Input to model: [tok₀, tok₁, tok₂, tok₃, tok₄]        ← all 5 tokens fed in\n",
    "\n",
    "W_key(x)   runs on 5 tokens → K₀, K₁, K₂, K₃, K₄     ← RECOMPUTED from scratch\n",
    "W_value(x) runs on 5 tokens → V₀, V₁, V₂, V₃, V₄     ← RECOMPUTED from scratch\n",
    "W_query(x) runs on 5 tokens → Q₀, Q₁, Q₂, Q₃, Q₄     ← RECOMPUTED from scratch\n",
    "\n",
    "Attention: all Q's @ all K's  → (5 × 5) score matrix   ← 25 dot products\n",
    "Softmax → weighted sum of V's → 5 context vectors\n",
    "\n",
    "But we only CARE about the last row (Q₄'s output) to predict the next token!\n",
    "The other 4 rows (Q₀-Q₃) are completely WASTED computation.\n",
    "```\n",
    "\n",
    "### With cache — generating the 5th token\n",
    "\n",
    "The model only processes the **1 new token**:\n",
    "\n",
    "```\n",
    "Input to model: [tok₄]                                  ← just 1 token!\n",
    "\n",
    "W_key(x)   runs on 1 token  → K₄                        ← only the new one\n",
    "W_value(x) runs on 1 token  → V₄                        ← only the new one\n",
    "W_query(x) runs on 1 token  → Q₄                        ← only the new one\n",
    "\n",
    "Cache already has: [K₀, K₁, K₂, K₃] from previous calls\n",
    "Append K₄, V₄ → cache now: [K₀, K₁, K₂, K₃, K₄]\n",
    "\n",
    "Attention: Q₄ @ all K's  → (1 × 5) score vector         ← 5 dot products (not 25!)\n",
    "Softmax → weighted sum of V's → 1 context vector\n",
    "```\n",
    "\n",
    "### Side-by-side comparison\n",
    "\n",
    "```\n",
    "                            Without Cache     With Cache      Savings\n",
    "                            ─────────────     ──────────      ───────\n",
    "W_key linear layer runs:    5 tokens          1 token         4 tokens saved ✓\n",
    "W_value linear layer runs:  5 tokens          1 token         4 tokens saved ✓\n",
    "W_query linear layer runs:  5 tokens          1 token         4 tokens saved ✓\n",
    "Attention score matrix:     5 × 5 = 25        1 × 5 = 5      20 ops saved ✓\n",
    "Context vectors computed:   5 (4 thrown away)  1 (the one      4 wasted vectors\n",
    "                                               we need)        avoided ✓\n",
    "```\n",
    "\n",
    "### The key insight\n",
    "\n",
    "The cache saves you from:\n",
    "1. **Recomputing K and V** for old tokens (read them from cache instead — this is the \"KV\" in \"KV cache\")\n",
    "2. **Computing Q for old tokens** (we don't need their queries — we only care about the new token's query)\n",
    "3. **Computing attention rows we'd throw away** (without cache, you get a 5×5 matrix but only use the last row)\n",
    "\n",
    "What the cache does **NOT** skip:\n",
    "- The new token's Q still does a dot product against **ALL** cached keys — that (1 × total_cached) multiplication happens every decode step\n",
    "- Softmax, scaling, and the weighted sum of values still happen every step\n",
    "\n",
    "This is why the line `attn_scores = queries @ keys.transpose(2, 3)` in the code above works identically in both cases — it's always \"queries dot-product with all keys.\" The difference is just whether `queries` has 5 rows or 1 row.\n",
    "\n",
    "### A helpful analogy\n",
    "\n",
    "Imagine you're a teacher grading essays:\n",
    "\n",
    "**Without cache (old way):** Every time a new student submits an essay, you re-read ALL previously submitted essays before reading the new one. You take notes on every essay, but then throw away all notes except the ones for the latest essay.\n",
    "\n",
    "**With cache:** You keep your notes from all previous essays in a folder (the cache). When a new essay arrives, you only read THAT one essay, add notes to your folder, then compare your new notes against ALL your previous notes to give a grade. You still look through all your notes (the dot product) — but you don't re-read the old essays (no recomputation of K, V).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the Causal Mask is Trickier with a KV Cache\n",
    "\n",
    "In Chapter 3, we learned about the **causal mask** — it prevents tokens from \"peeking\" at future tokens during attention. Without the KV cache, building this mask was simple. With the cache, it's not — and understanding why is key to understanding the code.\n",
    "\n",
    "### Without cache (old way) — simple!\n",
    "\n",
    "All tokens are processed together, so Q and K have the **same number of rows**. The mask is a square upper-triangular matrix:\n",
    "\n",
    "```\n",
    "Processing all at once: [tok₀, tok₁, tok₂, tok₃]\n",
    "\n",
    "Q has 4 rows, K has 4 rows → attention score matrix is (4 × 4)\n",
    "\n",
    "Mask (✓ = can attend, ✗ = blocked):\n",
    "\n",
    "          K₀   K₁   K₂   K₃\n",
    "   Q₀  [  ✓    ✗    ✗    ✗  ]   ← tok₀ only sees itself\n",
    "   Q₁  [  ✓    ✓    ✗    ✗  ]   ← tok₁ sees tok₀, tok₁\n",
    "   Q₂  [  ✓    ✓    ✓    ✗  ]   ← tok₂ sees tok₀, tok₁, tok₂\n",
    "   Q₃  [  ✓    ✓    ✓    ✓  ]   ← tok₃ sees everything before it\n",
    "\n",
    "Code: mask = torch.triu(torch.ones(4, 4), diagonal=1)\n",
    "      ↑ that's it! One line. The matrix is square, diagonal starts at 1.\n",
    "```\n",
    "\n",
    "### With cache (during decode) — the matrix isn't even square!\n",
    "\n",
    "During decode, we have **1 query** (the new token) but the keys include **all cached tokens**. The attention matrix is `(1 × total_cached)`, not square:\n",
    "\n",
    "```\n",
    "Cache has: [K₀, K₁, K₂, K₃]   ← 4 tokens from prefill\n",
    "New token: [tok₄]              ← 1 new token, Q₄ is the only query\n",
    "\n",
    "Q has 1 row, K has 5 columns → attention score matrix is (1 × 5)\n",
    "\n",
    "          K₀   K₁   K₂   K₃   K₄\n",
    "   Q₄  [  ✓    ✓    ✓    ✓    ✓  ]   ← tok₄ can see EVERYTHING (it comes last)\n",
    "\n",
    "torch.triu DOESN'T WORK here — the matrix isn't square!\n",
    "You can't build an upper-triangular mask on a (1 × 5) matrix.\n",
    "```\n",
    "\n",
    "In this case, the mask is all ✓ (no blocking) because tok₄ is the latest token — it should be able to attend to all previous tokens. Simple enough.\n",
    "\n",
    "### With cache (during prefill) — offset matters!\n",
    "\n",
    "It gets more interesting during prefill when we process multiple tokens. Imagine we process 3 tokens, but 2 were already cached:\n",
    "\n",
    "```\n",
    "Cache before: [K₀, K₁]          ← 2 tokens already cached\n",
    "New tokens:   [tok₂, tok₃, tok₄] ← 3 new tokens being prefilled\n",
    "\n",
    "Q has 3 rows, K has 5 columns → attention score matrix is (3 × 5)\n",
    "\n",
    "          K₀   K₁   K₂   K₃   K₄\n",
    "   Q₂  [  ✓    ✓    ✓    ✗    ✗  ]   ← tok₂ sees tok₀-₂, NOT tok₃ or tok₄\n",
    "   Q₃  [  ✓    ✓    ✓    ✓    ✗  ]   ← tok₃ sees tok₀-₃, NOT tok₄\n",
    "   Q₄  [  ✓    ✓    ✓    ✓    ✓  ]   ← tok₄ sees everything\n",
    "\n",
    "Notice: Q₂ is NOT the 0th token in the sequence — it's the 2nd!\n",
    "The mask diagonal needs to be SHIFTED by 2 (the offset).\n",
    "```\n",
    "\n",
    "### The offset trick in code\n",
    "\n",
    "This is what these three lines in `MultiHeadAttention.forward()` do:\n",
    "\n",
    "```python\n",
    "offset = K - num_tokens                                    # = 5 - 3 = 2\n",
    "row_idx = torch.arange(num_tokens, device=x.device).unsqueeze(1)  # [0, 1, 2] (query positions)\n",
    "col_idx = torch.arange(K, device=x.device).unsqueeze(0)           # [0, 1, 2, 3, 4] (key positions)\n",
    "causal_mask = (row_idx + offset) < col_idx                 # True = blocked\n",
    "```\n",
    "\n",
    "Let's trace through it for the example above (3 new queries, 5 total keys, offset=2):\n",
    "\n",
    "```\n",
    "row_idx + offset:  [0+2, 1+2, 2+2] = [2, 3, 4]   ← real positions of our queries\n",
    "\n",
    "Comparison: (row_idx + offset) < col_idx\n",
    "\n",
    "              col_idx →    0      1      2      3      4\n",
    "row+offset=2          [  2<0?   2<1?   2<2?   2<3?   2<4?  ]   = [F, F, F, T, T] → block col 3,4 ✓\n",
    "row+offset=3          [  3<0?   3<1?   3<2?   3<3?   3<4?  ]   = [F, F, F, F, T] → block col 4   ✓\n",
    "row+offset=4          [  4<0?   4<1?   4<2?   4<3?   4<4?  ]   = [F, F, F, F, F] → block nothing ✓\n",
    "\n",
    "This gives the exact mask we want:\n",
    "          K₀   K₁   K₂   K₃   K₄\n",
    "   Q₂  [  ✓    ✓    ✓    ✗    ✗  ]\n",
    "   Q₃  [  ✓    ✓    ✓    ✓    ✗  ]\n",
    "   Q₄  [  ✓    ✓    ✓    ✓    ✓  ]\n",
    "```\n",
    "\n",
    "### Why `torch.triu` can't do this\n",
    "\n",
    "`torch.triu` only works on **square** matrices and always puts the diagonal starting from position (0,0). With the KV cache:\n",
    "- The matrix is often **not square** (1 query × many keys during decode)\n",
    "- The diagonal needs to be **shifted** by the number of previously cached tokens\n",
    "- The offset changes every decode step as the cache grows\n",
    "\n",
    "That's why the code uses the `row_idx + offset < col_idx` formula instead — it handles all three cases (prefill from empty, prefill with existing cache, single-token decode) with one simple expression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Building Blocks (Chapter 4)\n",
    "\n",
    "Now that we understand the attention mechanism with its KV cache (the hard part!), the remaining pieces of the transformer are unchanged from Chapter 4. These components process each token's representation **after** attention has mixed information across tokens.\n",
    "\n",
    "Quick refresher on what each does:\n",
    "\n",
    "- **LayerNorm**: Normalizes each token's embedding to have mean=0, variance=1. Helps training stability. Think of it as \"resetting the scale\" before each sub-layer.\n",
    "- **GELU**: The activation function (like ReLU but smoother). Adds non-linearity so the network can learn complex patterns.\n",
    "- **FeedForward**: A two-layer MLP that processes each token independently. If attention is \"tokens talking to each other\", FeedForward is \"each token thinking on its own.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Normalizes activations across the embedding dimension.\n",
    "    Makes training more stable by keeping values in a reasonable range.\"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5  # Small constant to avoid division by zero\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))   # Learnable scale (starts at 1)\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))  # Learnable shift (starts at 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)  # Normalize to mean=0, var=1\n",
    "        return self.scale * norm_x + self.shift             # Then scale and shift (learned)\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"Gaussian Error Linear Unit — a smooth activation function.\n",
    "    Unlike ReLU which has a hard cutoff at 0, GELU has a soft curve,\n",
    "    which helps with gradient flow during training.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Two-layer MLP: expand to 4x the embedding size, apply GELU, then project back.\n",
    "    This is where each token 'thinks' independently (no interaction between tokens here).\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),  # Expand: 768 → 3072\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),  # Contract: 3072 → 768\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlock & GPTModel\n",
    "\n",
    "### TransformerBlock\n",
    "Each block follows the same pattern: **Norm → Attention → Add Residual → Norm → FeedForward → Add Residual**\n",
    "\n",
    "The residual connections (\"shortcuts\") add the input back to the output. This helps with training deep networks — if a layer can't learn anything useful, the residual lets the signal pass through unchanged.\n",
    "\n",
    "### GPTModel — the key change for KV cache\n",
    "\n",
    "Without a cache, position embeddings are simple: token at index 0 gets position 0, index 1 gets position 1, etc.\n",
    "\n",
    "With a cache, we need to **remember where we left off**. If we prefilled 4 tokens (positions 0-3), then the next generated token should get position 4, not position 0.\n",
    "\n",
    "That's what `ptr_current_pos` does — it tracks the next position to assign.\n",
    "\n",
    "```\n",
    "Prefill \"Hello, I am\" → positions [0, 1, 2, 3], ptr_current_pos becomes 4\n",
    "Decode next token     → position  [4],           ptr_current_pos becomes 5\n",
    "Decode next token     → position  [5],           ptr_current_pos becomes 6\n",
    "```\n",
    "\n",
    "Also notice we use `nn.ModuleList` instead of `nn.Sequential` so we can loop through blocks manually — `nn.Sequential` doesn't let us pass extra info between blocks if we ever needed to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"One transformer block = Attention + FeedForward, each with LayerNorm and residual connections.\n",
    "\n",
    "    The data flow through one block:\n",
    "\n",
    "        Input x\n",
    "          │\n",
    "          ├──────────────┐ (save for residual)\n",
    "          ▼              │\n",
    "        LayerNorm        │\n",
    "          ▼              │\n",
    "        Attention (KV cached) │\n",
    "          ▼              │\n",
    "        Dropout          │\n",
    "          ▼              │\n",
    "        + ◄──────────────┘ (add residual — \"shortcut connection\")\n",
    "          │\n",
    "          ├──────────────┐ (save for residual)\n",
    "          ▼              │\n",
    "        LayerNorm        │\n",
    "          ▼              │\n",
    "        FeedForward      │\n",
    "          ▼              │\n",
    "        Dropout          │\n",
    "          ▼              │\n",
    "        + ◄──────────────┘ (add residual)\n",
    "          │\n",
    "        Output\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "            window_size=cfg.get(\"kv_window_size\", cfg[\"context_length\"])\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Attention sub-block with residual ---\n",
    "        shortcut = x              # Save input for the residual connection\n",
    "        x = self.norm1(x)         # Normalize before attention (Pre-LayerNorm)\n",
    "        x = self.att(x)           # Multi-head attention — KV cache handles the rest\n",
    "        x = self.drop_shortcut(x) # Dropout for regularization\n",
    "        x = x + shortcut          # Add residual: output = attention(x) + x\n",
    "\n",
    "        # --- FeedForward sub-block with residual ---\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)            # Each token processes independently through the MLP\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"The full GPT model: Token Embeddings + Position Embeddings → N Transformer Blocks → Output Logits.\n",
    "\n",
    "    This ties together everything from Chapters 2-4 of \"Build a Large Language Model From Scratch\":\n",
    "      - Ch 2: Tokenization and data loading (GPTDatasetV1, tiktoken)\n",
    "      - Ch 3: The attention mechanism (MultiHeadAttention — now with KV cache)\n",
    "      - Ch 4: The full GPT architecture (this class)\n",
    "\n",
    "    The KV cache changes two things here:\n",
    "      1. We use ModuleList (not Sequential) so we can loop through blocks explicitly\n",
    "      2. We track ptr_current_pos to give each token the correct position embedding,\n",
    "         even when tokens arrive one at a time across multiple forward() calls\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embedding: converts token IDs (integers) into dense vectors\n",
    "        # Vocabulary of 50,257 tokens, each mapped to a 768-dim vector\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        # Position embedding: encodes WHERE a token is in the sequence\n",
    "        # Position 0 gets one vector, position 1 gets another, etc.\n",
    "        # This is how the model knows word order (attention itself is order-agnostic)\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Stack of transformer blocks — GPT-2 124M has 12 of these\n",
    "        # Each block refines the token representations through attention + feedforward\n",
    "        self.trf_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        # Output head: projects from embedding space (768) back to vocabulary space (50,257)\n",
    "        # The logits tell us how likely each token in the vocabulary is to come next\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "        self.kv_window_size = cfg.get(\"kv_window_size\", cfg[\"context_length\"])\n",
    "\n",
    "        # ptr_current_pos: tracks the absolute position in the sequence\n",
    "        # This is crucial for the KV cache — when we generate token by token,\n",
    "        # each new token needs to know its position in the overall sequence\n",
    "        self.ptr_current_pos = 0\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        # Step 1: Convert token IDs to embeddings\n",
    "        # \"Hello\" (token 15496) → [0.12, -0.34, 0.56, ...] (768 numbers)\n",
    "        tok_embeds = self.tok_emb(in_idx)  # (batch, seq_len, emb_dim)\n",
    "\n",
    "        # Step 2: Create position embeddings starting from ptr_current_pos\n",
    "        # Without cache: always start from position 0\n",
    "        # With cache: continue from where we left off\n",
    "        context_length = self.pos_emb.num_embeddings  # max positions (1024)\n",
    "        assert self.ptr_current_pos + seq_len <= context_length, (\n",
    "            f\"Position overflow: want position {self.ptr_current_pos + seq_len}, max is {context_length}\"\n",
    "        )\n",
    "\n",
    "        # Generate position IDs: e.g., if ptr_current_pos=4 and seq_len=1, pos_ids = [4]\n",
    "        pos_ids = torch.arange(\n",
    "            self.ptr_current_pos, self.ptr_current_pos + seq_len,\n",
    "            device=in_idx.device, dtype=torch.long\n",
    "        )\n",
    "        self.ptr_current_pos += seq_len  # Advance for the next call\n",
    "\n",
    "        pos_embeds = self.pos_emb(pos_ids).unsqueeze(0)  # (1, seq_len, emb_dim)\n",
    "\n",
    "        # Step 3: Combine token + position embeddings\n",
    "        # The model now knows WHAT each token is AND WHERE it is\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # Step 4: Pass through all transformer blocks\n",
    "        # Each block: Attention (with KV cache) → FeedForward → residual connections\n",
    "        for blk in self.trf_blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        # Step 5: Final layer norm + project to vocabulary logits\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)  # (batch, seq_len, vocab_size)\n",
    "        # logits[0, -1, :] = probability scores for the NEXT token after the last input token\n",
    "        return logits\n",
    "\n",
    "    def reset_kv_cache(self):\n",
    "        \"\"\"Reset all caches and position pointer — call before each new generation.\"\"\"\n",
    "        for blk in self.trf_blocks:\n",
    "            blk.att.reset_cache()\n",
    "        self.ptr_current_pos = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with KV Cache\n",
    "\n",
    "Generation happens in two distinct phases:\n",
    "\n",
    "### Phase 1: Prefill\n",
    "Feed the **entire prompt** through the model in one shot (or in chunks if the prompt is very long). This populates the KV cache with keys and values for every prompt token.\n",
    "\n",
    "```\n",
    "Prompt: \"Hello, I am\"  (4 tokens)\n",
    "→ model(\"Hello, I am\")\n",
    "→ Cache now holds K,V for all 4 tokens\n",
    "→ logits tell us what comes after \"am\"\n",
    "```\n",
    "\n",
    "### Phase 2: Decode (autoregressive loop)\n",
    "Generate **one token at a time**. Each step:\n",
    "1. Take the logits from the previous step, pick the most likely next token\n",
    "2. Feed **only that one token** into the model\n",
    "3. The model computes Q, K, V for just that token, appends K,V to cache, and attends to the full cached history\n",
    "4. Get new logits, repeat\n",
    "\n",
    "```\n",
    "Step 1: model(\"_a\")      → cache: [Hello, ,, I, am, _a]      → predicts \" large\"\n",
    "Step 2: model(\"_large\")  → cache: [Hello, ,, I, am, _a, _large] → predicts \" language\"\n",
    "...\n",
    "```\n",
    "\n",
    "This is where the KV cache saves time: without it, step 2 would need to process `[\"Hello\", \",\", \"I\", \"am\", \" a\", \" large\"]` — all 6 tokens. With the cache, it only processes `[\" large\"]` — 1 token — and reads the other 5 from cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_cached(model, idx, max_new_tokens, context_size=None):\n",
    "    \"\"\"Generate tokens one at a time using the KV cache.\n",
    "\n",
    "    Args:\n",
    "        model:          The GPT model (with KV cache built in)\n",
    "        idx:            Starting token IDs, shape (batch, prompt_length)\n",
    "        max_new_tokens: How many new tokens to generate\n",
    "        context_size:   Max sequence length (defaults to model's context_length)\n",
    "\n",
    "    Returns:\n",
    "        Token IDs tensor with the prompt + all generated tokens appended\n",
    "    \"\"\"\n",
    "    model.eval()  # Turn off dropout (we're generating, not training)\n",
    "\n",
    "    ctx_len = context_size or model.pos_emb.num_embeddings  # 1024 for GPT-2\n",
    "    kv_window_size = model.kv_window_size\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed during generation (saves memory)\n",
    "\n",
    "        # ============ PHASE 1: PREFILL ============\n",
    "        # Process the entire prompt to populate the KV cache.\n",
    "        # If the prompt is longer than kv_window_size, process it in chunks.\n",
    "        model.reset_kv_cache()  # Start fresh — clear any leftover cache\n",
    "\n",
    "        input_tokens = idx[:, -ctx_len:]  # Truncate prompt if it exceeds context length\n",
    "        input_tokens_length = input_tokens.size(1)\n",
    "\n",
    "        # Feed prompt through the model (populates cache with K,V for all prompt tokens)\n",
    "        for i in range(0, input_tokens_length, kv_window_size):\n",
    "            chunk = input_tokens[:, i:i + kv_window_size]\n",
    "            logits = model(chunk)\n",
    "            # After this loop, the cache holds K,V for the entire prompt\n",
    "            # and 'logits' contains predictions for what comes after the last prompt token\n",
    "\n",
    "        # ============ PHASE 2: DECODE (one token at a time) ============\n",
    "        # We can generate at most (context_length - prompt_length) new tokens\n",
    "        # because position embeddings have a fixed size\n",
    "        max_generable = ctx_len - input_tokens_length\n",
    "        max_new_tokens = min(max_new_tokens, max_generable)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Pick the most likely next token from the logits\n",
    "            # logits[:, -1] = predictions for the position after the last token\n",
    "            # argmax picks the token with the highest score (greedy decoding)\n",
    "            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "            # Append the new token to our running sequence\n",
    "            idx = torch.cat([idx, next_idx], dim=1)\n",
    "\n",
    "            # Feed ONLY the new token into the model\n",
    "            # The cache already has K,V for all previous tokens —\n",
    "            # we just need to compute K,V for this one new token and append it\n",
    "            logits = model(next_idx)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together — Run the Model\n",
    "\n",
    "Now we create a GPT-2 124M model (random weights — not pretrained) and generate text. The output will be gibberish since we haven't trained the model, but the architecture and KV cache mechanism are exactly the same as the real GPT-2.\n",
    "\n",
    "### GPT-2 124M Configuration\n",
    "| Parameter | Value | Meaning |\n",
    "|-----------|-------|---------|\n",
    "| vocab_size | 50,257 | Number of unique tokens (words/subwords) the model knows |\n",
    "| context_length | 1,024 | Maximum sequence length the model can handle |\n",
    "| emb_dim | 768 | Size of each token's internal representation |\n",
    "| n_heads | 12 | Number of attention heads (each looks at different patterns) |\n",
    "| n_layers | 12 | Number of transformer blocks stacked on top of each other |\n",
    "| drop_rate | 0.1 | 10% dropout for regularization during training |\n",
    "| kv_window_size | 1,024 | How many tokens the KV cache can hold (= context_length here) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 124M configuration — same architecture as OpenAI's smallest GPT-2\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,      # BPE vocabulary size (Ch 2: Byte Pair Encoding tokenizer)\n",
    "    \"context_length\": 1024,   # Max sequence length the model can process\n",
    "    \"emb_dim\": 768,           # Embedding dimension (each token = 768-dim vector)\n",
    "    \"n_heads\": 12,            # Number of attention heads (Ch 3: Multi-Head Attention)\n",
    "    \"n_layers\": 12,           # Number of transformer blocks (Ch 4: full GPT architecture)\n",
    "    \"drop_rate\": 0.1,         # Dropout rate (10%) for regularization\n",
    "    \"qkv_bias\": False,        # No bias in Q, K, V projections (GPT-2 style)\n",
    "    \"kv_window_size\": 1024,   # KV cache window = context_length (no sliding window)\n",
    "}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create the model and move to GPU if available\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Switch to evaluation mode (disables dropout)\n",
    "\n",
    "print(f\"Model on: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "# ~124 million parameters — hence \"GPT-2 124M\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text using tiktoken (Ch 2: BPE tokenizer)\n",
    "# This converts human-readable text into token IDs that the model understands\n",
    "start_context = \"Hello, I am\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")  # Use GPT-2's tokenizer\n",
    "encoded = tokenizer.encode(start_context)   # \"Hello, I am\" → [15496, 11, 314, 716]\n",
    "encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(\"Input text:\", start_context)\n",
    "print(\"Encoded token IDs:\", encoded)\n",
    "print(\"Tensor shape:\", encoded_tensor.shape)  # (1, 4) = batch of 1, prompt of 4 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the KV cache!\n",
    "# The model has random weights so the output will be gibberish,\n",
    "# but this demonstrates the full generation pipeline:\n",
    "#   1. Prefill: \"Hello, I am\" → cache K,V for 4 tokens\n",
    "#   2. Decode: generate 200 tokens one at a time, each reading from cache\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate_text_cached(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "total_time = time.time() - start\n",
    "\n",
    "# Decode token IDs back to human-readable text\n",
    "decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "\n",
    "print(f\"Output length: {len(token_ids[0])} tokens (4 prompt + 200 generated)\")\n",
    "print(f\"Time: {total_time:.2f} sec\")\n",
    "print(f\"Speed: {int(len(token_ids[0])/total_time)} tokens/sec\")\n",
    "print(f\"\\nGenerated text:\\n{decoded_text}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    max_mem_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
    "    print(f\"\\nMax GPU memory: {max_mem_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond the Code: How the KV Cache Powers Safety in Production LLMs\n",
    "\n",
    "Everything we built above isn't just an academic exercise — the KV cache is at the heart of how production LLMs like ChatGPT, Claude, and Gemini enforce their safety behavior. Here's the connection.\n",
    "\n",
    "### System Prompts: Safety Instructions as Tokens\n",
    "\n",
    "When you chat with an LLM through an API or app, the model doesn't just see your message. Behind the scenes, a **system prompt** (sometimes called a \"safety prompt\") is prepended before your input. It's just regular text — plain tokens — that tells the model how to behave.\n",
    "\n",
    "```\n",
    "What actually enters the model:\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│ [System Prompt Tokens]   [User Prompt Tokens]   [Generation]   │\n",
    "│                                                                 │\n",
    "│ \"You are a helpful        \"Hello, I am\"          \" a helpful    │\n",
    "│  assistant. You must                               AI assistant │\n",
    "│  not produce harmful                               made by...\"  │\n",
    "│  content. Always be                                             │\n",
    "│  respectful and...\"                                             │\n",
    "│                                                                 │\n",
    "│  (could be 500-2000+      (your message)         (model output) │\n",
    "│   tokens!)                                                      │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### System Prompts Live in the KV Cache\n",
    "\n",
    "During the **prefill phase** (the first `for` loop in our `generate_text_cached` function), the system prompt tokens are processed just like any other tokens. Their Keys and Values are computed and written into `cache_k` and `cache_v`.\n",
    "\n",
    "From that point on, **every single generated token attends to the system prompt's K,V** through the attention mechanism. This is how the model \"remembers\" its safety instructions throughout an entire conversation — the system prompt isn't magic, it's just tokens sitting in the KV cache that every new token pays attention to.\n",
    "\n",
    "```python\n",
    "# What really happens inside generate_text_cached in production:\n",
    "\n",
    "# Phase 1: Prefill — system prompt + user prompt get cached\n",
    "full_prompt = system_prompt_tokens + user_message_tokens\n",
    "logits = model(full_prompt)  # K,V for ALL tokens (including safety instructions) now in cache\n",
    "\n",
    "# Phase 2: Decode — every generated token attends to the cached system prompt\n",
    "for _ in range(max_tokens):\n",
    "    next_token = logits[:, -1].argmax(...)\n",
    "    logits = model(next_token)  # This 1 token's query attends to ALL cached K,V\n",
    "                                 # including the system prompt's K,V\n",
    "                                 # → the model \"sees\" its safety instructions on every step\n",
    "```\n",
    "\n",
    "### The Real-World Cost: System Prompts Eat Your Context Window\n",
    "\n",
    "This has a direct practical impact. If the system prompt is 1,000 tokens and your context window is 4,096 tokens:\n",
    "\n",
    "```\n",
    "Total context window:     4,096 tokens\n",
    "System prompt:           -1,000 tokens (safety instructions)\n",
    "Available for you:        3,096 tokens (your conversation + generated output)\n",
    "\n",
    "That's ~25% of the context used up before you type a single word!\n",
    "```\n",
    "\n",
    "This is why companies invest heavily in optimizing this. The KV cache mechanisms we learned above directly enable these real-world optimizations:\n",
    "\n",
    "### Industry Optimization 1: Prompt Caching\n",
    "\n",
    "The system prompt is **identical** across millions of requests. So why recompute its K,V every time?\n",
    "\n",
    "**Prompt caching** (used by Anthropic, OpenAI, and others) pre-computes the system prompt's Keys and Values once and reuses them across requests. The first user pays the prefill cost; subsequent users with the same system prompt skip it entirely.\n",
    "\n",
    "```\n",
    "User A: [system prompt K,V computed] + \"What is Python?\" → response\n",
    "User B: [system prompt K,V REUSED]   + \"Tell me a joke\" → response  ← saved ~1000 tokens of compute!\n",
    "User C: [system prompt K,V REUSED]   + \"Write an email\" → response  ← saved again!\n",
    "```\n",
    "\n",
    "This is exactly the same cache mechanism from our `MultiHeadAttention.forward()` — just shared across users instead of across decode steps.\n",
    "\n",
    "### Industry Optimization 2: Prefix Sharing\n",
    "\n",
    "Multiple concurrent conversations that start with the same system prompt can **share the same KV cache memory on the GPU**, rather than duplicating it per request. With thousands of simultaneous users, this saves gigabytes of GPU memory.\n",
    "\n",
    "```\n",
    "GPU Memory without prefix sharing:\n",
    "  User A: [system_KV (copy 1)] + [user_A_KV]     ← 1000 tokens of K,V duplicated\n",
    "  User B: [system_KV (copy 2)] + [user_B_KV]     ← same 1000 tokens again\n",
    "  User C: [system_KV (copy 3)] + [user_C_KV]     ← and again...\n",
    "\n",
    "GPU Memory with prefix sharing:\n",
    "  Shared: [system_KV (1 copy)]                    ← stored once\n",
    "  User A:                       + [user_A_KV]     ← only unique K,V per user\n",
    "  User B:                       + [user_B_KV]\n",
    "  User C:                       + [user_C_KV]\n",
    "```\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "The KV cache we implemented in this notebook is the **same fundamental mechanism** that powers safety, efficiency, and scalability in production LLMs. System prompts aren't a separate safety system — they're just tokens whose Keys and Values live in the cache, influencing every generated token through the attention mechanism we coded in `MultiHeadAttention.forward()`.\n",
    "\n",
    "Understanding this connection helps explain:\n",
    "- **Why LLMs sometimes \"forget\" safety instructions** in very long conversations (the system prompt's influence gets diluted as the cache fills with conversation tokens)\n",
    "- **Why context window size matters so much** (system prompt + conversation must all fit)\n",
    "- **Why API providers charge differently for cached vs. uncached tokens** (the compute cost is genuinely different)\n",
    "- **Why prompt injection attacks are a concern** (adversarial user tokens in the cache compete for attention with the system prompt tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT-KV (Python 3.11)\n",
   "language": "python",
   "name": "gpt-kv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
