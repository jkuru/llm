{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE) — A Beginner's Walkthrough\n",
    "\n",
    "*A companion to the [KV Cache Walkthrough](kv_cache_always_on.ipynb) and [LoRA Notebook](lora_on_device.ipynb) — based on Sebastian Raschka's [\"Build a Large Language Model From Scratch\"](https://www.manning.com/books/build-a-large-language-model-from-scratch).*\n",
    "\n",
    "In the KV cache notebook we optimized **inference speed** (don't recompute old K,V). In the LoRA notebook we optimized **customization** (tiny adapters instead of full fine-tuning). This notebook tackles a different question:\n",
    "\n",
    "> **How do you make a model smarter without making every forward pass slower?**\n",
    "\n",
    "The answer is **Mixture of Experts (MoE)** — the architecture behind models like Mixtral 8x7B, GPT-4, and DeepSeek-V2.\n",
    "\n",
    "### The core idea\n",
    "\n",
    "Instead of one large FeedForward network that processes every token, MoE has **multiple smaller \"expert\" networks** and a **router (gate)** that picks which experts to use for each token. Most experts are idle on any given token — so you get the knowledge capacity of a huge model with the compute cost of a small one.\n",
    "\n",
    "```\n",
    "Standard FFN (Dense):                    MoE FFN (Sparse):\n",
    "                                         \n",
    "token → [  BIG FFN  ] → output           token → [Router] → picks top-2 experts\n",
    "         (always runs)                                │\n",
    "                                              ┌──────┼──────────┐\n",
    "                                              ▼      ▼          │\n",
    "                                         [Expert 1][Expert 3]  [Expert 2,4,5,6,7,8]\n",
    "                                          (active)  (active)    (IDLE — not computed!)\n",
    "                                              │      │\n",
    "                                              └──┬───┘\n",
    "                                                 ▼\n",
    "                                          weighted sum → output\n",
    "```\n",
    "\n",
    "> **How to use this notebook:** Run cells top-to-bottom (`Shift+Enter`). We build a working MoE FeedForward layer, compare it to the standard dense FFN from Chapter 4, and see how it fits into the GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Recap: The Standard (Dense) FeedForward\n",
    "\n",
    "In Chapter 4, we built this FeedForward network that sits inside every TransformerBlock:\n",
    "\n",
    "```\n",
    "Input x (768) → Linear (768 → 3072) → GELU → Linear (3072 → 768) → Output (768)\n",
    "                     expand 4x                    contract back\n",
    "```\n",
    "\n",
    "Every token goes through the **same** FFN, using **all** the parameters. This is called a **dense** architecture — dense because every parameter is active on every forward pass.\n",
    "\n",
    "The problem: if you want a smarter model, you make the FFN bigger (wider or deeper). But bigger FFN = more computation on **every single token**. This gets expensive fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense FFN parameters: 4,722,432\n",
      "  Layer 1: (768 × 3072) + 3072 bias = 2,362,368\n",
      "  Layer 2: (3072 × 768) + 768 bias  = 2,360,064\n",
      "  All 4,722,432 are active on EVERY token\n"
     ]
    }
   ],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"\"\"Gaussian Error Linear Unit — smooth activation function (Ch 4).\"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Standard dense FFN from Chapter 4.\n",
    "    Every token uses ALL parameters on every forward pass.\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"]),   # 768 → 3072\n",
    "            GELU(),\n",
    "            nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"]),   # 3072 → 768\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Quick parameter count\n",
    "cfg_dense = {\"emb_dim\": 768, \"hidden_dim\": 768 * 4}\n",
    "dense_ffn = FeedForward(cfg_dense)\n",
    "dense_params = sum(p.numel() for p in dense_ffn.parameters())\n",
    "print(f\"Dense FFN parameters: {dense_params:,}\")\n",
    "print(f\"  Layer 1: (768 × 3072) + 3072 bias = {768*3072 + 3072:,}\")\n",
    "print(f\"  Layer 2: (3072 × 768) + 768 bias  = {3072*768 + 768:,}\")\n",
    "print(f\"  All {dense_params:,} are active on EVERY token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MoE FeedForward: Multiple Experts + a Router\n",
    "\n",
    "MoE replaces the single dense FFN with:\n",
    "\n",
    "1. **Multiple expert networks** — each expert is a small FFN (same architecture as the dense one, but can be smaller)\n",
    "2. **A router (gate)** — a simple linear layer that scores each expert for each token, then picks the top-k\n",
    "\n",
    "### How the router works\n",
    "\n",
    "```\n",
    "Input token x (768-dim)\n",
    "    │\n",
    "    ▼\n",
    "Router: nn.Linear(768, num_experts)    ← one score per expert\n",
    "    │\n",
    "    ▼\n",
    "Scores: [0.1, 2.3, -0.5, 1.8, 0.4, -1.2, 0.7, 0.9]    ← 8 experts\n",
    "    │\n",
    "    ▼\n",
    "Top-2:  Expert 1 (score 2.3), Expert 3 (score 1.8)       ← pick the best 2\n",
    "    │\n",
    "    ▼\n",
    "Softmax over top-2 scores: [0.62, 0.38]                  ← normalize to weights\n",
    "    │\n",
    "    ▼\n",
    "Output = 0.62 × Expert_1(x) + 0.38 × Expert_3(x)        ← weighted combination\n",
    "```\n",
    "\n",
    "### Why this is efficient\n",
    "\n",
    "With 8 experts but top-2 routing, each token only runs through **2 out of 8** expert networks. That's 25% of the expert compute, but the model has 8x the total expert parameters to store knowledge in.\n",
    "\n",
    "### The SwiGLU activation\n",
    "\n",
    "The experts in this implementation use **SwiGLU** instead of GELU — a gated activation that modern LLMs (LLaMA, Mixtral, etc.) prefer:\n",
    "\n",
    "```\n",
    "Standard:  output = GELU(fc1(x))                    ← one path\n",
    "SwiGLU:    output = SiLU(fc1(x)) * fc2(x)           ← two paths multiplied (gating)\n",
    "```\n",
    "\n",
    "The gate (`fc2`) learns to selectively filter what information passes through — like a learned attention mechanism within the FFN itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoEFeedForward class defined!\n"
     ]
    }
   ],
   "source": [
    "class MoEFeedForward(nn.Module):\n",
    "    \"\"\"Mixture of Experts FeedForward layer.\n",
    "    \n",
    "    Instead of one big FFN, we have multiple small \"expert\" FFNs.\n",
    "    A router picks the top-k experts for each token.\n",
    "    Only the selected experts run — the rest are skipped entirely.\n",
    "    \n",
    "    Architecture of each expert (SwiGLU style):\n",
    "        hidden = SiLU(fc1(x)) * fc2(x)    ← gated activation\n",
    "        output = fc3(hidden)                ← project back to emb_dim\n",
    "    \n",
    "    Args (from cfg dict):\n",
    "        emb_dim:             Embedding dimension (768 for GPT-2)\n",
    "        hidden_dim:          Expert hidden size (typically 4x emb_dim)\n",
    "        num_experts:         Total number of experts (e.g., 8)\n",
    "        num_experts_per_tok: How many experts to use per token (e.g., 2)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_experts_per_tok = cfg[\"num_experts_per_tok\"]\n",
    "        self.num_experts = cfg[\"num_experts\"]\n",
    "        self.emb_dim = cfg[\"emb_dim\"]\n",
    "\n",
    "        # ========== THE ROUTER (GATE) ==========\n",
    "        # A simple linear layer that produces one score per expert for each token\n",
    "        # Input: token embedding (768) → Output: expert scores (num_experts)\n",
    "        # No bias — we don't want the router to have a default preference\n",
    "        self.gate = nn.Linear(cfg[\"emb_dim\"], cfg[\"num_experts\"], bias=False)\n",
    "\n",
    "        # ========== THE EXPERTS ==========\n",
    "        # Each expert has 3 linear layers (SwiGLU architecture):\n",
    "        #   fc1: \"gate path\"   — (emb_dim → hidden_dim), activated with SiLU\n",
    "        #   fc2: \"value path\"  — (emb_dim → hidden_dim), multiplied with fc1's output\n",
    "        #   fc3: \"output path\" — (hidden_dim → emb_dim), projects back\n",
    "        self.fc1 = nn.ModuleList(\n",
    "            [nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], bias=False)\n",
    "             for _ in range(self.num_experts)]\n",
    "        )\n",
    "        self.fc2 = nn.ModuleList(\n",
    "            [nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], bias=False)\n",
    "             for _ in range(self.num_experts)]\n",
    "        )\n",
    "        self.fc3 = nn.ModuleList(\n",
    "            [nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], bias=False)\n",
    "             for _ in range(self.num_experts)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, emb_dim)\n",
    "        batch, seq_len, _ = x.shape\n",
    "\n",
    "        # ========== STEP 1: ROUTE — decide which experts handle which tokens ==========\n",
    "        # Gate produces a score for each expert, for each token\n",
    "        scores = self.gate(x)  # (batch, seq_len, num_experts)\n",
    "\n",
    "        # Pick the top-k experts with the highest scores\n",
    "        topk_scores, topk_indices = torch.topk(\n",
    "            scores, self.num_experts_per_tok, dim=-1\n",
    "        )  # both: (batch, seq_len, num_experts_per_tok)\n",
    "\n",
    "        # Softmax over ONLY the top-k scores → these become the mixing weights\n",
    "        # (we don't softmax over ALL experts — only the selected ones)\n",
    "        topk_probs = torch.softmax(topk_scores, dim=-1)\n",
    "        # topk_probs: (batch, seq_len, num_experts_per_tok)\n",
    "        # e.g., [0.62, 0.38] meaning 62% weight on expert 1, 38% on expert 3\n",
    "\n",
    "        # ========== STEP 2: FLATTEN for efficient expert dispatch ==========\n",
    "        # Merge batch and seq_len dims: (batch * seq_len, emb_dim)\n",
    "        x_flat = x.reshape(batch * seq_len, -1)\n",
    "        out_flat = torch.zeros(\n",
    "            batch * seq_len, self.emb_dim, device=x.device, dtype=x.dtype\n",
    "        )\n",
    "\n",
    "        topk_indices_flat = topk_indices.reshape(-1, self.num_experts_per_tok)\n",
    "        topk_probs_flat = topk_probs.reshape(-1, self.num_experts_per_tok)\n",
    "\n",
    "        # ========== STEP 3: RUN EACH ACTIVE EXPERT ==========\n",
    "        # Find which experts were actually selected by at least one token\n",
    "        unique_experts = torch.unique(topk_indices_flat)\n",
    "\n",
    "        for expert_id_tensor in unique_experts:\n",
    "            expert_id = int(expert_id_tensor.item())\n",
    "\n",
    "            # Find which tokens selected this expert\n",
    "            mask = topk_indices_flat == expert_id  # (total_tokens, num_experts_per_tok)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            # Get indices of tokens that use this expert\n",
    "            token_mask = mask.any(dim=-1)  # (total_tokens,) — True if token uses this expert\n",
    "            selected_idx = token_mask.nonzero(as_tuple=False).squeeze(-1)\n",
    "            if selected_idx.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # Gather the input for just these tokens\n",
    "            expert_input = x_flat.index_select(0, selected_idx)\n",
    "\n",
    "            # ===== SwiGLU computation =====\n",
    "            # gate_path = SiLU(fc1(x))  ← smooth gating\n",
    "            # value_path = fc2(x)        ← what to pass through\n",
    "            # hidden = gate_path * value_path  ← element-wise gating\n",
    "            # output = fc3(hidden)       ← project back to emb_dim\n",
    "            hidden = F.silu(self.fc1[expert_id](expert_input)) * \\\n",
    "                     self.fc2[expert_id](expert_input)\n",
    "            expert_out = self.fc3[expert_id](hidden)\n",
    "\n",
    "            # Get the routing probability for this expert\n",
    "            # (each token may have selected this expert in slot 0 or slot 1 of top-k)\n",
    "            mask_selected = mask[selected_idx]\n",
    "            slot_indices = mask_selected.int().argmax(dim=-1, keepdim=True)\n",
    "            selected_probs = torch.gather(\n",
    "                topk_probs_flat.index_select(0, selected_idx),\n",
    "                dim=-1, index=slot_indices\n",
    "            ).squeeze(-1)\n",
    "\n",
    "            # Accumulate: output += expert_output * routing_weight\n",
    "            # index_add_ scatters the results back to the correct token positions\n",
    "            out_flat.index_add_(\n",
    "                0, selected_idx, expert_out * selected_probs.unsqueeze(-1)\n",
    "            )\n",
    "\n",
    "        # ========== STEP 4: RESHAPE back to (batch, seq_len, emb_dim) ==========\n",
    "        return out_flat.reshape(batch, seq_len, self.emb_dim)\n",
    "\n",
    "\n",
    "print(\"MoEFeedForward class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing Through an Example\n",
    "\n",
    "Let's walk through what happens to a single token step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Router Decision for One Token ===\n",
      "\n",
      "All expert scores: [0.17712165415287018, 0.24847665429115295, 0.74974524974823, -0.02854180708527565, 0.4913672208786011, 0.8514304161071777, -0.6397891640663147, -0.4104754328727722]\n",
      "\n",
      "Selected experts: [5, 2]\n",
      "Selected scores:  [0.8514304161071777, 0.74974524974823]\n",
      "Routing weights (after softmax): [0.5253994464874268, 0.474600613117218]\n",
      "\n",
      "Expert activity:\n",
      "  Expert 0: |##########          | score=+0.177  [idle]\n",
      "  Expert 1: |###########         | score=+0.248  [idle]\n",
      "  Expert 2: |#############       | score=+0.750  [ACTIVE]\n",
      "  Expert 3: |#########           | score=-0.029  [idle]\n",
      "  Expert 4: |############        | score=+0.491  [idle]\n",
      "  Expert 5: |##############      | score=+0.851  [ACTIVE]\n",
      "  Expert 6: |######              | score=-0.640  [idle]\n",
      "  Expert 7: |#######             | score=-0.410  [idle]\n",
      "\n",
      "→ Only 2/8 experts computed!\n",
      "  Compute savings: 75%\n"
     ]
    }
   ],
   "source": [
    "# Create an MoE layer with 8 experts, top-2 routing\n",
    "cfg_moe = {\n",
    "    \"emb_dim\": 768,\n",
    "    \"hidden_dim\": 768 * 4,  # 3072 — same hidden size as dense FFN\n",
    "    \"num_experts\": 8,\n",
    "    \"num_experts_per_tok\": 2,\n",
    "}\n",
    "\n",
    "torch.manual_seed(42)\n",
    "moe_ffn = MoEFeedForward(cfg_moe)\n",
    "\n",
    "# Create a single token embedding\n",
    "x = torch.randn(1, 1, 768)  # batch=1, seq_len=1, emb_dim=768\n",
    "\n",
    "# Step 1: See what the router produces\n",
    "with torch.no_grad():\n",
    "    scores = moe_ffn.gate(x)  # (1, 1, 8) — one score per expert\n",
    "    topk_scores, topk_indices = torch.topk(scores, 2, dim=-1)\n",
    "    topk_probs = torch.softmax(topk_scores, dim=-1)\n",
    "\n",
    "print(\"=== Router Decision for One Token ===\")\n",
    "print(f\"\\nAll expert scores: {scores.squeeze().tolist()}\")\n",
    "print(f\"\\nSelected experts: {topk_indices.squeeze().tolist()}\")\n",
    "print(f\"Selected scores:  {topk_scores.squeeze().tolist()}\")\n",
    "print(f\"Routing weights (after softmax): {topk_probs.squeeze().tolist()}\")\n",
    "\n",
    "# Visualize which experts are active vs idle\n",
    "selected = set(topk_indices.squeeze().tolist())\n",
    "print(f\"\\nExpert activity:\")\n",
    "for i in range(8):\n",
    "    status = \"ACTIVE\" if i in selected else \"idle\"\n",
    "    score = scores.squeeze()[i].item()\n",
    "    bar = \"|\" + \"#\" * max(0, int((score + 2) * 5)) + \" \" * max(0, 20 - int((score + 2) * 5)) + \"|\"\n",
    "    print(f\"  Expert {i}: {bar} score={score:+.3f}  [{status}]\")\n",
    "\n",
    "print(f\"\\n→ Only {len(selected)}/{cfg_moe['num_experts']} experts computed!\")\n",
    "print(f\"  Compute savings: {(1 - len(selected)/cfg_moe['num_experts'])*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count: Dense vs MoE\n",
    "\n",
    "This is the key insight — MoE has **more total parameters** but uses **fewer per token**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config                       Total Params  Active Params/Token   Active %\n",
      "---------------------------------------------------------------------------\n",
      "Dense FFN                       4,722,432            4,722,432       100%\n",
      "MoE 4E top-1                   28,314,624            7,080,960        25%\n",
      "MoE 8E top-2                   56,629,248           14,161,920        25%\n",
      "MoE 16E top-2                 113,258,496           14,168,064        13%\n",
      "\n",
      "Key insight:\n",
      "  MoE 8E top-2 has ~8x the parameters of dense FFN\n",
      "  But only ~25% are active per token — similar compute to dense!\n",
      "  The extra 75% idle parameters still STORE knowledge\n",
      "  → more capacity, similar speed\n"
     ]
    }
   ],
   "source": [
    "# Dense FFN\n",
    "dense_ffn = FeedForward({\"emb_dim\": 768, \"hidden_dim\": 3072})\n",
    "dense_total = sum(p.numel() for p in dense_ffn.parameters())\n",
    "\n",
    "# MoE FFN with 8 experts\n",
    "moe_configs = [\n",
    "    {\"num_experts\": 4, \"num_experts_per_tok\": 1},\n",
    "    {\"num_experts\": 8, \"num_experts_per_tok\": 2},\n",
    "    {\"num_experts\": 16, \"num_experts_per_tok\": 2},\n",
    "]\n",
    "\n",
    "print(f\"{'Config':<25} {'Total Params':>15} {'Active Params/Token':>20} {'Active %':>10}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Dense FFN':<25} {dense_total:>15,} {dense_total:>20,} {'100%':>10}\")\n",
    "\n",
    "for mc in moe_configs:\n",
    "    cfg_tmp = {\"emb_dim\": 768, \"hidden_dim\": 3072, **mc}\n",
    "    moe_tmp = MoEFeedForward(cfg_tmp)\n",
    "    total = sum(p.numel() for p in moe_tmp.parameters())\n",
    "    \n",
    "    # Each expert has: fc1(768*3072) + fc2(768*3072) + fc3(3072*768) = 3 * 768 * 3072\n",
    "    expert_params = 3 * 768 * 3072  # params per expert\n",
    "    active_params = expert_params * mc[\"num_experts_per_tok\"] + 768 * mc[\"num_experts\"]  # + router\n",
    "    active_pct = f\"{active_params/total*100:.0f}%\"\n",
    "    \n",
    "    label = f\"MoE {mc['num_experts']}E top-{mc['num_experts_per_tok']}\"\n",
    "    print(f\"{label:<25} {total:>15,} {active_params:>20,} {active_pct:>10}\")\n",
    "\n",
    "print()\n",
    "print(\"Key insight:\")\n",
    "print(\"  MoE 8E top-2 has ~8x the parameters of dense FFN\")\n",
    "print(\"  But only ~25% are active per token — similar compute to dense!\")\n",
    "print(\"  The extra 75% idle parameters still STORE knowledge\")\n",
    "print(\"  → more capacity, similar speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Different Tokens Get Different Experts\n",
    "\n",
    "The router learns to send different types of tokens to different experts. In a trained model, you'd see patterns like:\n",
    "\n",
    "```\n",
    "Token: \"photosynthesis\"  → Expert 2 (science/biology specialist)\n",
    "Token: \"litigation\"      → Expert 5 (legal terminology specialist)  \n",
    "Token: \"the\"            → Expert 0 (common words / syntax specialist)\n",
    "Token: \"def\"            → Expert 7 (code/programming specialist)\n",
    "```\n",
    "\n",
    "This **specialization** is what gives MoE its power — each expert can focus on a specific type of knowledge without competing for capacity with everything else. A dense FFN would need to cram all of these specialties into a single set of weights.\n",
    "\n",
    "### The routing is learned, not hardcoded\n",
    "\n",
    "The gate (`nn.Linear(emb_dim, num_experts)`) is trained alongside the experts via backpropagation. Over time, it learns which experts are best at handling which types of input. Nobody tells it \"Expert 2 should handle science\" — it figures this out during training.\n",
    "\n",
    "Let's verify that different inputs do get routed differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which experts does each token use?\n",
      "──────────────────────────────────────────────────\n",
      "  Token 0 → Expert 5 and Expert 2\n",
      "  Token 1 → Expert 1 and Expert 0\n",
      "  Token 2 → Expert 6 and Expert 4\n",
      "  Token 3 → Expert 2 and Expert 1\n",
      "  Token 4 → Expert 3 and Expert 1\n",
      "  Token 5 → Expert 7 and Expert 3\n",
      "\n",
      "Expert utilization across all 6 tokens:\n",
      "  Expert 0: █      (1 tokens)\n",
      "  Expert 1: ███    (3 tokens)\n",
      "  Expert 2: ██     (2 tokens)\n",
      "  Expert 3: ██     (2 tokens)\n",
      "  Expert 4: █      (1 tokens)\n",
      "  Expert 5: █      (1 tokens)\n",
      "  Expert 6: █      (1 tokens)\n",
      "  Expert 7: █      (1 tokens)\n",
      "\n",
      "Note: In a trained model, the router would learn meaningful specialization.\n",
      "With random weights, the routing is essentially random — that's expected!\n"
     ]
    }
   ],
   "source": [
    "# Create multiple different token embeddings\n",
    "torch.manual_seed(42)\n",
    "moe_ffn = MoEFeedForward(cfg_moe)\n",
    "\n",
    "# Simulate 6 different tokens (random embeddings — in a trained model these would be meaningful)\n",
    "tokens = torch.randn(1, 6, 768)\n",
    "\n",
    "with torch.no_grad():\n",
    "    scores = moe_ffn.gate(tokens)  # (1, 6, 8)\n",
    "    topk_scores, topk_indices = torch.topk(scores, 2, dim=-1)\n",
    "\n",
    "print(\"Which experts does each token use?\")\n",
    "print(\"─\" * 50)\n",
    "for t in range(6):\n",
    "    experts = topk_indices[0, t].tolist()\n",
    "    print(f\"  Token {t} → Expert {experts[0]} and Expert {experts[1]}\")\n",
    "\n",
    "print()\n",
    "# Count how often each expert is used\n",
    "expert_usage = torch.zeros(8)\n",
    "for t in range(6):\n",
    "    for e in topk_indices[0, t].tolist():\n",
    "        expert_usage[e] += 1\n",
    "\n",
    "print(\"Expert utilization across all 6 tokens:\")\n",
    "for i in range(8):\n",
    "    bar = \"█\" * int(expert_usage[i].item())\n",
    "    print(f\"  Expert {i}: {bar:<6} ({int(expert_usage[i].item())} tokens)\")\n",
    "\n",
    "print(f\"\\nNote: In a trained model, the router would learn meaningful specialization.\")\n",
    "print(f\"With random weights, the routing is essentially random — that's expected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plugging MoE into the GPT Architecture\n",
    "\n",
    "MoE replaces the FeedForward layer **inside each TransformerBlock**. Everything else stays the same — attention, layer norm, residual connections, KV cache — all unchanged.\n",
    "\n",
    "```\n",
    "TransformerBlock (Dense — Chapter 4):     TransformerBlock (MoE):\n",
    "\n",
    "    Input x                                   Input x\n",
    "      │                                         │\n",
    "    LayerNorm                                 LayerNorm\n",
    "      │                                         │\n",
    "    Attention (+ KV cache)                    Attention (+ KV cache)    ← SAME\n",
    "      │                                         │\n",
    "    + residual                                + residual                ← SAME\n",
    "      │                                         │\n",
    "    LayerNorm                                 LayerNorm                 ← SAME\n",
    "      │                                         │\n",
    "    FeedForward  ← one big FFN                MoE FeedForward  ← router + experts\n",
    "      │                                         │\n",
    "    + residual                                + residual                ← SAME\n",
    "      │                                         │\n",
    "    Output                                    Output\n",
    "```\n",
    "\n",
    "The swap is this simple in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:      [2, 10, 768]\n",
      "Dense FFN output: [2, 10, 768]\n",
      "MoE FFN output:   [2, 10, 768]\n",
      "\n",
      "→ Same shape! MoE is a drop-in replacement for the dense FFN.\n",
      "  The rest of the transformer (attention, norms, residuals) doesn't change.\n"
     ]
    }
   ],
   "source": [
    "# In the TransformerBlock from gpt_with_kv_cache.py, the only change is:\n",
    "\n",
    "# self.ff = FeedForward(cfg)                                    ← Dense (Chapter 4)\n",
    "# self.ff = MoEFeedForward(cfg) if cfg[\"num_experts\"] > 0 \\    ← MoE (when experts > 0)\n",
    "#           else FeedForward(cfg)                                ← Dense (fallback)\n",
    "\n",
    "# Let's verify both produce the same-shaped output\n",
    "x = torch.randn(2, 10, 768)  # batch=2, seq_len=10, emb_dim=768\n",
    "\n",
    "dense_out = dense_ffn(x)\n",
    "moe_out = moe_ffn(x)\n",
    "\n",
    "print(f\"Input shape:      {list(x.shape)}\")\n",
    "print(f\"Dense FFN output: {list(dense_out.shape)}\")\n",
    "print(f\"MoE FFN output:   {list(moe_out.shape)}\")\n",
    "print(f\"\\n→ Same shape! MoE is a drop-in replacement for the dense FFN.\")\n",
    "print(f\"  The rest of the transformer (attention, norms, residuals) doesn't change.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model Comparison: Dense vs MoE\n",
    "\n",
    "Let's build a full GPT-like model with both architectures and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full GPT Model Comparison ===\n",
      "\n",
      "                                     Dense GPT   MoE GPT (8E top-2)\n",
      "----------------------------------------------------------------------\n",
      "Total parameters                   163,009,536          785,891,328\n",
      "FFN parameters                      56,669,184          679,550,976\n",
      "Non-FFN parameters                 106,340,352          106,340,352\n",
      "FFN params active/token             56,669,184          169,887,744\n",
      "\n",
      "The MoE model has 4.8x more total parameters\n",
      "but activates roughly the same compute per token as dense.\n",
      "\n",
      "Think of it as: 786M parameters of KNOWLEDGE,\n",
      "with ~163M parameters of COMPUTE per token.\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / torch.sqrt(var + self.eps) + self.shift\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Simplified MHA (no KV cache) for clean comparison.\"\"\"\n",
    "    def __init__(self, d_in, d_out, num_heads, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, n, _ = x.shape\n",
    "        q = self.W_query(x).view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.W_key(x).view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.W_value(x).view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        scores = q @ k.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
    "        mask = torch.triu(torch.ones(n, n, device=x.device), diagonal=1).bool()\n",
    "        scores.masked_fill_(mask, -torch.inf)\n",
    "        weights = self.dropout(torch.softmax(scores, dim=-1))\n",
    "        out = (weights @ v).transpose(1, 2).contiguous().view(b, n, self.d_out)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"], qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        # THIS IS THE KEY LINE — MoE or Dense based on config\n",
    "        self.ff = MoEFeedForward(cfg) if cfg.get(\"num_experts\", 0) > 0 else FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.drop(self.att(self.norm1(x)))\n",
    "        x = x + self.drop(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        b, n = idx.shape\n",
    "        x = self.tok_emb(idx) + self.pos_emb(torch.arange(n, device=idx.device))\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.head(self.norm(x))\n",
    "\n",
    "\n",
    "# Build both models\n",
    "base_cfg = {\n",
    "    \"vocab_size\": 50257, \"context_length\": 1024, \"emb_dim\": 768,\n",
    "    \"n_heads\": 12, \"n_layers\": 12, \"drop_rate\": 0.0, \"qkv_bias\": False,\n",
    "    \"hidden_dim\": 3072,\n",
    "}\n",
    "\n",
    "# Dense model (standard GPT-2)\n",
    "dense_cfg = {**base_cfg, \"num_experts\": 0}\n",
    "torch.manual_seed(42)\n",
    "dense_model = GPTModel(dense_cfg)\n",
    "\n",
    "# MoE model (8 experts, top-2)\n",
    "moe_cfg = {**base_cfg, \"num_experts\": 8, \"num_experts_per_tok\": 2}\n",
    "torch.manual_seed(42)\n",
    "moe_model = GPTModel(moe_cfg)\n",
    "\n",
    "dense_params = sum(p.numel() for p in dense_model.parameters())\n",
    "moe_params = sum(p.numel() for p in moe_model.parameters())\n",
    "\n",
    "# Count just FFN params\n",
    "dense_ffn_params = sum(\n",
    "    sum(p.numel() for p in blk.ff.parameters()) for blk in dense_model.blocks\n",
    ")\n",
    "moe_ffn_params = sum(\n",
    "    sum(p.numel() for p in blk.ff.parameters()) for blk in moe_model.blocks\n",
    ")\n",
    "\n",
    "print(\"=== Full GPT Model Comparison ===\")\n",
    "print(f\"\")\n",
    "print(f\"{'':30} {'Dense GPT':>15} {'MoE GPT (8E top-2)':>20}\")\n",
    "print(f\"{'-'*70}\")\n",
    "print(f\"{'Total parameters':30} {dense_params:>15,} {moe_params:>20,}\")\n",
    "print(f\"{'FFN parameters':30} {dense_ffn_params:>15,} {moe_ffn_params:>20,}\")\n",
    "print(f\"{'Non-FFN parameters':30} {dense_params-dense_ffn_params:>15,} {moe_params-moe_ffn_params:>20,}\")\n",
    "print(f\"{'FFN params active/token':30} {dense_ffn_params:>15,} {moe_ffn_params//4:>20,}\")\n",
    "print(f\"\")\n",
    "print(f\"The MoE model has {moe_params/dense_params:.1f}x more total parameters\")\n",
    "print(f\"but activates roughly the same compute per token as dense.\")\n",
    "print(f\"\")\n",
    "print(f\"Think of it as: {moe_params/1e6:.0f}M parameters of KNOWLEDGE,\")\n",
    "print(f\"with ~{dense_params/1e6:.0f}M parameters of COMPUTE per token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing: Dense vs MoE Forward Pass\n",
    "\n",
    "Let's measure the actual computation time to verify MoE doesn't significantly slow down the forward pass despite having many more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass time (128 tokens, averaged over 5 runs):\n",
      "  Dense GPT (163M params): 38.8 ms\n",
      "  MoE GPT   (786M params):  119.3 ms\n",
      "  Ratio: 3.08x\n",
      "\n",
      "  Note: On CPU, MoE has some overhead from routing logic.\n",
      "  On GPU with proper batching, the gap narrows significantly.\n",
      "\n",
      "  On GPU with optimized kernels, MoE would be nearly the same speed\n",
      "  as dense while having 4.8x the capacity.\n"
     ]
    }
   ],
   "source": [
    "# Benchmark both models\n",
    "dense_model.eval()\n",
    "moe_model.eval()\n",
    "\n",
    "# Create a test input\n",
    "test_input = torch.randint(0, 50257, (1, 128))  # batch=1, seq_len=128\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    _ = dense_model(test_input)\n",
    "    _ = moe_model(test_input)\n",
    "\n",
    "# Time dense model\n",
    "n_runs = 5\n",
    "dense_times = []\n",
    "for _ in range(n_runs):\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        _ = dense_model(test_input)\n",
    "    dense_times.append(time.perf_counter() - start)\n",
    "\n",
    "# Time MoE model\n",
    "moe_times = []\n",
    "for _ in range(n_runs):\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        _ = moe_model(test_input)\n",
    "    moe_times.append(time.perf_counter() - start)\n",
    "\n",
    "dense_avg = sum(dense_times) / n_runs * 1000  # ms\n",
    "moe_avg = sum(moe_times) / n_runs * 1000  # ms\n",
    "\n",
    "print(f\"Forward pass time (128 tokens, averaged over {n_runs} runs):\")\n",
    "print(f\"  Dense GPT ({dense_params/1e6:.0f}M params): {dense_avg:.1f} ms\")\n",
    "print(f\"  MoE GPT   ({moe_params/1e6:.0f}M params):  {moe_avg:.1f} ms\")\n",
    "print(f\"  Ratio: {moe_avg/dense_avg:.2f}x\")\n",
    "print(f\"\")\n",
    "if moe_avg < dense_avg * 2:\n",
    "    print(f\"  Despite having {moe_params/dense_params:.1f}x more parameters,\")\n",
    "    print(f\"  MoE is only {moe_avg/dense_avg:.2f}x slower — because most experts are idle!\")\n",
    "else:\n",
    "    print(f\"  Note: On CPU, MoE has some overhead from routing logic.\")\n",
    "    print(f\"  On GPU with proper batching, the gap narrows significantly.\")\n",
    "print(f\"\")\n",
    "print(f\"  On GPU with optimized kernels, MoE would be nearly the same speed\")\n",
    "print(f\"  as dense while having {moe_params/dense_params:.1f}x the capacity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World MoE Models\n",
    "\n",
    "Here's how MoE is used in actual production models:\n",
    "\n",
    "```\n",
    "Model              Total Params    Active Params/Token    Experts    Top-k\n",
    "──────────────     ────────────    ───────────────────    ───────    ─────\n",
    "GPT-2 124M         124M            124M (dense)           1          1\n",
    "Mixtral 8x7B       46.7B           12.9B                  8          2\n",
    "DeepSeek-V2         236B            21B                   160         6\n",
    "GPT-4 (rumored)    ~1.8T           ~280B                  16         2\n",
    "```\n",
    "\n",
    "### Mixtral 8x7B: The MoE poster child\n",
    "\n",
    "Mixtral has 8 expert FFNs per layer, each the size of a 7B model's FFN. The router picks the top-2 for each token. So:\n",
    "- Total parameters: 46.7B (lots of stored knowledge)\n",
    "- Active parameters per token: 12.9B (affordable compute)\n",
    "- Result: Matches or beats LLaMA 2 70B despite using ~5x less compute per token\n",
    "\n",
    "### The tradeoff\n",
    "\n",
    "MoE isn't free — you need to store all expert parameters in memory, even though most are idle:\n",
    "\n",
    "```\n",
    "Dense 13B model:    13B params × 2 bytes (fp16) = 26 GB GPU memory\n",
    "Mixtral 8x7B:       46.7B params × 2 bytes      = 93 GB GPU memory\n",
    "                    (but only computes like a ~13B model!)\n",
    "```\n",
    "\n",
    "This is why MoE pairs well with **quantization** (shrink each parameter from 2 bytes to 0.5 bytes) and **offloading** (keep idle experts on CPU/disk, only load active ones to GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How MoE, KV Cache, and LoRA Work Together\n",
    "\n",
    "These three optimizations target different bottlenecks and combine naturally:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    The Modern LLM Optimization Stack                │\n",
    "│                                                                     │\n",
    "│  PROBLEM                SOLUTION              WHERE IN GPT          │\n",
    "│  ───────                ────────              ────────────          │\n",
    "│                                                                     │\n",
    "│  Model too slow         KV Cache              MultiHeadAttention    │\n",
    "│  at generation?         (don't recompute      (cache K,V from       │\n",
    "│                          old K,V)              previous tokens)     │\n",
    "│                                                                     │\n",
    "│  Model not smart        MoE                   FeedForward           │\n",
    "│  enough?                (more expert params,   (replace dense FFN   │\n",
    "│                          same compute)          with router+experts)│\n",
    "│                                                                     │\n",
    "│  Need to customize      LoRA                  W_query, W_value      │\n",
    "│  for a task?            (tiny adapters,        (add small A×B       │\n",
    "│                          freeze base)           alongside frozen W) │\n",
    "│                                                                     │\n",
    "│  Model too big          Quantization          All weight matrices   │\n",
    "│  for device?            (4-bit weights,        (compress 32-bit     │\n",
    "│                          smaller model)         floats to 4-bit)    │\n",
    "│                                                                     │\n",
    "│  All together: A quantized MoE model with KV cache and LoRA        │\n",
    "│  adapters = maximum capability, minimum resource usage              │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "MoE is a way to scale model **knowledge** without proportionally scaling **compute**. Instead of making the FeedForward layer bigger (which slows down every token), you add more expert FFNs and let a learned router pick the best ones per token. The result: models like Mixtral 8x7B that match 70B-parameter dense models while using only ~13B parameters of compute per token.\n",
    "\n",
    "The `MoEFeedForward` class in this notebook is the same architecture used in production. The only difference is scale — Mixtral uses 8 experts with 7B-parameter-sized FFNs, while we used 8 experts with GPT-2-sized FFNs. The routing logic, gating mechanism, and SwiGLU computation are identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT-KV (Python 3.11)\n",
   "language": "python",
   "name": "gpt-kv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
